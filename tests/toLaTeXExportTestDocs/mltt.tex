\documentclass[11pt, oneside]{article}



%% Packages

\usepackage{listings}

%% Standard packages
\usepackage{geometry}
\geometry{letterpaper}
\usepackage{changepage}  % for the adjustwidth environment
\usepackage{graphicx}    % for \includegraphics

%% Index, hyperref
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{hyperref}   % load before imakeidx
\usepackage{imakeidx}

%%%%
\usepackage[normalem]{ulem} % for \st
\usepackage{soul}           % for \hl and \sethlcolor
\usepackage{wrapfig}        % for wrapfigure

%% AMS
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{amscd}

\usepackage{fancyvrb} %% for inline verbatim
\usepackage{makeidx}

%% Chemistry
\usepackage[version=4]{mhchem} % for \ce



%% Commands

\newcommand{\hang}[1]{%
  {%
    \setlength{\leftskip}{1em}%
    \setlength{\hangindent}{1em}%
    \hangafter=1 %
    #1\ \vpace{4}%
  }%
}

\renewcommand{\labelitemi}{\scalebox{0.7}{\textbullet}}

% Dot box = 1em, gap = 1em → total = 2em
\newcommand{\compactItem}[1]{%
  \par
\noindent
  \hangindent=2em \hangafter=1%
  \makebox[1em][l]{\labelitemi}\hspace{1em}#1\par
}

\newcommand{\code}[1]{{\tt #1}}
\newcommand{\ellie}[1]{\href{#1}{Link to Ellie}}
% \newcommand{\image}[3]{\includegraphics[width=3cm]{#1}}

%% width=4truein,keepaspectratio]


% imagecentercaptioned command removed - using standard figure environment instead

\newcommand{\imagecenter}[2]{
   \medskip
   \begin{figure}[htp]
   \centering
    \includegraphics[width=#2]{#1}
    \vglue0pt
    \end{figure}
    \medskip
}

\newcommand{\imagefloat}[4]{
    \begin{wrapfigure}{#4}{#2}
    \includegraphics[width=#2]{#1}
    \caption{#3}
    \end{wrapfigure}
}


\newcommand{\imagefloatright}[3]{
    \begin{wrapfigure}{R}{0.30\textwidth}
    \includegraphics[width=0.30\textwidth]{#1}
    \caption{#2}
    \end{wrapfigure}
}

\newcommand{\hide}[1]{}


\newcommand{\imagefloatleft}[3]{
    \begin{wrapfigure}{L}{0.3-\textwidth}
    \includegraphics[width=0.30\textwidth]{#1}
    \caption{#2}
    \end{wrapfigure}
}
% Font style
\newcommand{\italic}[1]{{\sl #1}}
\newcommand{\strong}[1]{{\bf #1}}
\newcommand{\strike}[1]{\st{#1}}

% Scripta
\newcommand{\ilink}[2]{\href{{https://scripta.io/s/#1}}{#2}}
\newcommand{\markwith}[1]{}
\newcommand{\anchor}[1]{#1}

% Color
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\violet}[1]{\textcolor{violet}{#1}}
\newcommand{\highlight}[1]{\hl{#1}}
\newcommand{\note}[2]{\textcolor{blue}{#1}{\hl{#1}}}

% WTF?
\newcommand{\remote}[1]{\textcolor{red}{#1}}
\newcommand{\local}[1]{\textcolor{blue}{#1}}

% Unclassified
\newcommand{\subheading}[1]{{\bf #1}\par}
%\newcommand{\term}[1]{{\index{#1}}}
%\newcommand{\termx}[1]{}
\newcommand{\comment}[1]{}
\newcommand{\innertableofcontents}{}


% Special character
\newcommand{\dollarSign}[0]{{\$}}
\newcommand{\backTick}[0]{\`{}}

%% Theorems
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{axiom}{Axiom}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{problem}{Problem}
\newtheorem{exercises}{Exercises}
\newcommand{\bs}[1]{$\backslash$#1}
\newcommand{\texarg}[1]{\{#1\}}


%% Environments
\renewenvironment{quotation}
  {\begin{adjustwidth}{2cm}{} \footnotesize}
  {\end{adjustwidth}}

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist

\renewenvironment{indent}
  {\begin{adjustwidth}{0.75cm}{}}
  {\end{adjustwidth}}


%% NEWCOMMAND

% \definecolor{mypink1}{rgb}{0.858, 0.188, 0.478}
% \definecolor{mypink2}{RGB}{219, 48, 122}
\newcommand{\fontRGB}[4]{
    \definecolor{mycolor}{RGB}{#1, #2, #3}
    \textcolor{mycolor}{#4}
    }

\newcommand{\highlightRGB}[4]{
    \definecolor{mycolor}{RGB}{#1, #2, #3}
    \sethlcolor{mycolor}
    \hl{#4}
     \sethlcolor{yellow}
    }

\newcommand{\gray}[2]{
\definecolor{mygray}{gray}{#1}
\textcolor{mygray}{#2}
}

\newcommand{\white}[1]{\gray{1}[#1]}
\newcommand{\medgray}[1]{\gray{0.5}[#1]}
\newcommand{\black}[1]{\gray{0}[#1]}

% Spacing
\parindent0pt
\parskip5pt

\makeindex[
                          title=Index,
                          columns=2,
                          %% intoc     % include index in the table of contents
                        ]

\begin{document}

\title{Untitled}

\date{}

\author{
test-author
}

\maketitle

\tableofcontents

%%% Line 6
\newcommand{\set}[1]{\{\ #1 \ \}}
\newcommand{\sett}[2]{\{\ #1 \ |\ #2 \}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\type}{\mathop{\mathsf{type}}}
\newcommand{\ctx}{\mathop{\mathsf{ctx}}}
\newcommand{\zero}{\mathop{\mathsf{zero}}}
\newcommand{\suc}{\mathop{\mathsf{succ}}}
\newcommand{\boolean}{\mathbb{B}}
\newcommand{\true}{\mathop{\mathsf{true}}}
\newcommand{\false}{\mathop{\mathsf{false}}}
\newcommand{\rec}{\mathop{\mathsf{rec}}}
\newcommand{\nott}{\mathop{\mathsf{not}}}
\newcommand{\and}{\mathop{\mathsf{and}}}
\newcommand{\or}{\mathop{\mathsf{or}}}
\newcommand{\toa}{\ensuremath{\to\!\!*\,}}
\newcommand{\cU}{\mathcal{U}}
\newcommand{\refl}{\mathop{\mathsf{refl}}}
\newcommand{\double}{\mathop{\mathsf{double}}}
\newcommand{\fact}{\mathop{\mathsf{fact}}}
\newcommand{\add}{\mathop{\mathsf{add}}}
\newcommand{\base}{\mathop{\mathsf{base}}}
\newcommand{\step}{\mathop{\mathsf{step}}}
\newcommand{\emptyt}{\bot}
\newcommand{\unitt}{\top}
\newcommand{\congg}{\mathop{\mathsf{cong}}}
\newcommand{\isRightIdentity}{\mathop{\mathsf{isRightIdentity}}}
\newcommand{\lemma}{\mathop{\mathsf{lemma}}}
\newcommand{\sym}{\mathop{\mathsf{sym}}}
\newcommand{\trans}{\mathop{\mathsf{trans}}}
\newcommand{\isEven}{\mathop{\mathsf{isEven}}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\bR}{\mathbb R}
\newcommand{\cP}{\mathcal P}
\newcommand{\ind}{\mathop{\mathsf{ind}}}



%%% Line 3


%%% Line 6


%%% Line 47


%%% Line 50
\section{Sets} \label{sets}

%%% Line 52
A set is a collection of elements.   Suppose that we
have   things like this: \(4\heartsuit, 5\diamondsuit, 6\clubsuit, 2\spadesuit\). We can gather them in to various collections, e.g.,

%%% Line 55
\begin{align}
A = \set{ 2\heartsuit, 5\heartsuit}\\
B = \set{2\heartsuit, 7\spades}
\end{align}

%%% Line 59
It makes sense to say \(e \in A\), meaning \quote{the element\(e\) is in the set \(A\)}.   Thus we have 

%%% Line 62
\begin{equation}
2\heartsuit \in A, \ 2\heartsuit\in B,\ 7\spadesuit \not\in A,\ 7\spadesuit \in B
\end{equation}

%%% Line 65
Assertions like \(2\heartsuit \in A\) or \(7\spadesuit \in A\) are \index{propositions}\textit{propositions}: they are capable of being true or false.   True in the first case, but false in the second.

%%% Line 67
Two important observations: the existence of the elements \(2\heartsuit, 5\heartsuit\), etc. is prior to the existence of the sets \(A\), \(B\), etc.   Also, a thing like \(2\heartsuit\) can be an element of more than one set, e.g., \(A\) and \(B\).

%%% Line 69
\subsection{Encoding things as sets} \label{encoding-things-as-sets}

%%% Line 71
Mathematical objects like the natural numbers can be described in the language of sets.   Here is one way to do   it. Encode zero as
the empty set \(\set{}\).
Encode one as \(\set{\set{}}\).   Encode two as 
\(\set{ \set{}, \set{\set{}}}\), etc. Introduce aliases for these numbers:

%%% Line 76
\begin{align}
&0 :\equiv \set{}\\
&1 :\equiv \set{\set{}}\\
&2 :\equiv \set{ \set{}, \set{\set{}}}\\
&etc.
\end{align}

%%% Line 82
Then we find \(0 \in 1\), \(0 \in 2\) and \(1 \in 2\), etc.   However,
none of these elements is element of itself: \(0 \not\in 0\), \(1 \not\in 1\), etc.

%%% Line 85
We can gather these numbers into sets in various ways: the set of all natural numbers,   \(\nat = \set{0, 1, 2, \ldots}{}\), the subsets of \(\tt{Even}\) and \(\tt{Odd}\) numbers, respectively.   The latter is described using a predicate --- a function like \(\text{isOdd}\) that returns true or false according to whether the argument of the
function is even or odd.

%%% Line 88
\begin{equation}
\tt{Odd} = \sett{x \in \nat}{\text{ isOdd(x)} = \text{\true}}
\end{equation}

%%% Line 91
Now consider the set   whose elments are not members of themselves:

%%% Line 93
\begin{equation}
R = \sett{x}{x \not\in x}
\end{equation}

%%% Line 96
Its definition is vaguely like that of \(\tt{Odd}\).
The numbers \(0, 1, 2, \dots\) are elements of \(R\), as 
are the sets \(\nat\), \(\tt{Odd}\), and \(\tt{Even}\).   But now ask if \(R\) is an element of itself.   If \(R \in R\), we conclude from the definition of \(R\), that \(R \not \in R\).   If \(R \in R\), we conclude
from the definition of \(R\), that \(R \in R\).   In either case, we find a contradiction, and so we are faced with a paradox.   We will revisit this
paradox in section \ref{comparing-sets-and-types}, where we discuss the origins of type theory and introdue the notion of type universes.

%%% Line 104
\section{Formal Systems} \label{formal-systems}

%%% Line 106
Martin-Löf type theory is a formal system, the two main ingredients
of which are \index{judgments}\textit{judgments} and \index{inference rules}\textit{inference rules}.   Judgments
are the things we assert.   They are the facts known to the system.
Rules of inference are rules use to derive new judgments from
existing ones.   Their general form is 

%%% Line 113
\begin{equation}
\frac{P_1 \ P_2 \ \ldots \ P_n}{C}
\end{equation}

%%% Line 117
where the \(P_i\) are the premises and \(C\) is the conclusion.   If
the premises are judgments, then so is the conclusion. The conclusion of a rule with no premises is a judgment.

%%% Line 121
As an example, we consider a version of Goncharov's Cherry-Banana Calculus.   It is a system with symbols \(\heartsuit\) and \(\spadesuit\) and the following inference rules:

%%% Line 124
\begin{equation}
\frac{}{\heartsuit}a\qquad \frac{X}{\spadesuit X}p\qquad\frac{X\spadesuit \quad \spadesuit Y}{XY}c\qquad  \frac{X}{\heartsuit X \spadesuit}
\end{equation}

%%% Line 131
Here the labels are a = axiom, p = prepend, c = collapse, and 
e = enclose   As it stands, the only judgment in this theory
is \(\heartsuit\). Here is a derivation of the judgment 
\(\heartsuit\heartsuit\heartsuit\):

%%% Line 136
\begin{equation}
\dfrac{\dfrac{\dfrac{}{\heartsuit}a}{\heartsuit\heartsuit\spadesuit}e\qquad\dfrac{\dfrac{}{\heartsuit}a}{\spadesuit\heartsuit}p}{\heartsuit\heartsuit\heartsuit}c
\end{equation}

%%% Line 140
If we include the intermediate judgments in the previous derivation, the judgments of the theory are now 

%%% Line 142
\begin{equation}
J_1, J_2, J_3, \ldots = 
\heartsuit,\ \heartsuit\heartsuit\spadesuit,\ 
\spadesuit\heartsuit,\ \heartsuit\heartsuit\heartsuit
\end{equation}

%%% Line 147
where each \(J_n\) is derived from judgments \(J_m\) with \(m < n\).
Not all strings of symbols are derivable.   The string \(\spadesuit\) is an example.   We make think of the derivable strings as the theorems of the formal system. The sequence \(\mathcal{J} = J_1, J_2, J_3, \ldots\) is the current state of knowledge. Note that \(\mathcal{J}\) is a dynamic entity: it grows as more inference rules are applied.

%%% Line 150
\section{MLTT as a Formal System} \label{mltt-as-a-formal-system}

%%% Line 152
There are five kinds of jugments in MLTT.

\begin{enumerate}

%%% Line 154
\item \(\Gamma \ctx\) --- \(\Gamma\) is a well-formed context, that is, a sequence of variable declarations like \(x_1 : A_1, \ldots x_n : A_n\), where latter types \(A_i\) may depend on earlier variables. Something like \(x: A, y: B\) where \(A\) depends on \(y\) is not well formed, nor is \(A, B, C\). In general, well-formed means \quote{formed according to the rules.}

%%% Line 157
\item \(\Gamma \vdash A\ \type\) --- under the assumptions of \(\Gamma\), \(A\) is a type

%%% Line 159
\item \(\Gamma \vdash t : A\) --- under the assumptions of \(\Gamma\), \(t\) is a term of type \(A\)

%%% Line 161
\item \(\Gamma \vdash A \equiv B\ \type\) --- under the assumptions of \(\Gamma\), \(A\) and \(B\) are equal types

%%% Line 163
\item \(\Gamma \vdash a \equiv b: A\) --- under the assumptions of \(\Gamma\), \(a\) and \(b\) are equal terms of type \(A\). We will see what this means in what follows.

\end{enumerate}

%%% Line 167
In the last two judgment forms, equality is \index{definitional equality}\textit{definitional equality} or \index{judgmental equality}\textit{judgmental equality}, i.e., equality according to the rules of computation and definition of the theory.

%%% Line 169
To define a type, we must give four kinds of inference rules: formation, introduction, elimination, and computation.   We begin with the first two rules for the natural numbers.

%%% Line 171
\begin{equation}
\frac{\Gamma \ctx}{\ \Gamma \vdash  \nat\  }\ f\qquad\frac{\Gamma \ctx}{\Gamma \vdash \zero : \nat}\ i_1\qquad\frac{ \Gamma \vdash k : \nat}{\Gamma \vdash \suc k : \nat}\ i_2
\end{equation}

%%% Line 179
The first rule is the formation rule for the natural numbers.   It does nothing more than announce that \(\nat\) is a type in the context \(\Gamma\).   The second two rules are introduction rules.   The first of these says that \(\zero\) is a term of \(\nat\) in context \(\Gamma\).   
The other says that if \(k\) is a term of \(\nat\) in context \(\Gamma\),
then so is \(\suc k\). 

%%% Line 183
By repeated 
application of the last rule, we produce arbitrarily many terms of \(\nat\): \(\zero\), \(\suc \zero\), \(\suc (\suc \zero)\), etc. The symbols \(\zero\) and \(\suc\) are the \index{constructors}\textit{constructors} of the natural numbers.

%%% Line 188
\begin{box}
The empty type, written \(\bot\), is a type with no introduction
rules, hence no constructors.   Consequently there are no terms of the empty type.
\end{box}

%%% Line 192
\textbf{Contexts.} There are many cases in which   we
%%% Line 195
\begin{equation}
\label{rules-without-\gamma}\frac{}{\   \nat\  }\ f\qquad\frac{}{\zero : \nat}\ i_1\qquad\qquad\frac{  k : \nat}{ \suc k : \nat}\ i_2
\end{equation}
can work with an empty context, in which case we might write the rules in simplified form as

%%% Line 204
We will frequently do this to cut down on visual complexity.   For more about contexts, see the box at the end of this section.

%%% Line 206
Let us continue with our discussion of the natural numbers.   Just as with the Cherry-Banana Calculus, judgments \(n : \nat\) are given by derivations, e.g.,

%%% Line 208
\begin{equation}
\dfrac{\dfrac{\dfrac{}{\zero : \nat}i_1}{\suc \zero: \nat}i_2}{\suc(\suc \zero) : \nat}i_2
\end{equation}

%%% Line 211
At this point our store of judgments is

%%% Line 213
\begin{equation}
\mathcal{J} = \nat\ \type,\ \zero : \nat,\ \suc\ \zero : \nat,\suc\ (\suc \zero) : \nat
\end{equation}

%%% Line 217
Of course it is cumbersome to work with expressions like 
\(\suc\ (\suc\ \zero)\), so we can make definitions
like \(0 :≡ \zero\), \(1   :≡ \suc \zero\), \(2 :≡ \suc\ (\suc \zero)\), 
etc.   Then it makes perfect sense to say that   \(3 ≡ \suc 2\).
This is a \index{definitional}\textit{definitional} or \index{judgmental}\textit{judgmental} equality.   Later we will study the far more sophisticated notion of
\index{propositional equality}\textit{propositional equality}.

%%% Line 225
\begin{box}
Consider the assertion \(\suc x : \nat\).   This is meaningless without the
   assumption \(x : \nat\).   Thus we write \(x : \nat \vdash \suc \nat\).   
   The context is \(x : \nat\).
\//{}   
Contexts are also needed to enforce theory-wide rules that demand that judgments be stable under \index{substitution}\textit{substitution} and \index{weakening}\textit{weakening}, i.e., if a term is well-typed in one context, it remains well-typed when new assumptions are added (weakening), or substitutions of variables are made.
\//{}
To reduce visual clutter, we will often write rules in the 
simplified form \eqref{rules-without-gamma}.   However, always
keep in mind that in these cases there is an invisible empty context. It is sometimes written as \(.\)
\//{}
Weakening and substitution are \textbf{structural rules} of MLTT.   They do not define particular types or terms, but rather describe
how judgments behave when we manipulate the context.   They are essential to making sure that the theory is \textbf{stable} and \textbf{compositional}: that building up more complex terms does not
break existing well-formedness.
\//{}
\//{}
\//{}
\textbf{Substitution.} Let's look at an example, then give the formal               rule.   Suppose that \(3 : \nat\) and 
\(x : \nat \vdash \suc x :\nat\).
Then by the subsitution rule, \(\vdash \suc 3 : \nat\)   
\//{}
\textbf{Substitution rule.}Suppose that \(\Gamma \vdash a : A\) and \(\Gamma, x : A, \Delta \vdash J\).   Then \(\Gamma, \Delta[a/x] \vdash J[a/x]\).   The notation \(Q[a/x]\) 
means \quote{replace every occurrence of\(x\) in \(Q\) by \(a\).}   The
expresssion   \(\Gamma, x : A, \Delta\) is the concatenation of of the contexts \(\Gamma\), \(x : A\), and \(\Delta\).
\//{}
\//{}
\//{}
\textbf{Weakening.} The idea is that if a judgment holds in some context,
then it holds if you add more assumptions, even if they are unused. The rule is that if \(\Gamma \vdash J\) and \(\Gamma\vdash   A   \type\)
then \(\Gamma, x : A \vdash J\). In the rule, we have added the variable \(x\) of type \(A\).   The judgment \(J\) remains valid.
\end{box}

%%% Line 256
\section{Lambda Calculus} \label{lambda-calculus}

%%% Line 258
The lambda calculus is the world's smallest programming language.
Nonetheless, it is Turing complete: it can compute anything
that any other programming language can compute, be it a Turing machine, C, Haskell, or any other.   A valid expresson in the lambda calculus is called a \(\lambda\)-term.   The rules for their formation
are as follow:

\begin{enumerate}

%%% Line 263
\item \textbf{Variable Rule.} There is an infinite list of variables \(x_1, x_2, x_3, \ldots\) These are \(\lambda\)-terms.

%%% Line 265
\item \textbf{Abstraction Rule.} If \(x\) is a variable and \(b\) is a lambda-term, then \(\lambda x.b\) is a \(\lambda\)-term.   We call \(b\) the boddy of the lambfda term.   We will think of abstractions as nameless (anonymous) functions.

%%% Line 269
\item \textbf{Application Rule.}   If \(a\) \(b\) are \(\lambda\)-terms, then so is \(a\, b\).   We will think of an appliation as being function   application

\end{enumerate}

%%% Line 273
To give meaning to the lambda calculus, we define the operation of \index{beta-reduction}\textit{beta-reduction}.   It defines what happens when an abstraction 
is applied to another \(\lambda\)-term:

%%% Line 276
\begin{equation}
\label{\beta-reduction}(\lambda x.b)a = b[a/x]
\end{equation}

%%% Line 281
where \(b[a/x]\) means that all occurrences of \(a\) in \(b\) are replaced by \(x\).   As an example, we have

%%% Line 283
\begin{equation}
(\lambda x.(x + 1))7 \xrightarrow{\beta} (x + 1)[7/x] = 7 + 1 = 8
\end{equation}

%%% Line 286
\textbf{Examples}

%%% Line 288
\((\lambda x.x)a \xrightarrow{\beta} x[x/a] = a\).   This lambda
expressoion acts as the identity function.

%%% Line 291
\begin{align}
(\lambda x. \lambda y  . x)\, 1\, 2 &= (\lambda x. \lambda y  . x)\, 1\, 2\\
& \xrightarrow{\beta} (\lambda y . 1)\, 2\\
& \xrightarrow{\beta}\, 1
\end{align}

%%% Line 296
\begin{align}
(\lambda x. \lambda y  . y)\, 1\, 2 &= (\lambda x. \lambda y  . x)\, 1\, 2\\
& \xrightarrow{\beta} (\lambda y . y)\, 2\\
& \xrightarrow{\beta}\, 2
\end{align}

%%% Line 303
\section{Function Types} \label{function-types}

%%% Line 305
Let us define the type of functions out of a type \(A\) and into a
type \(B\).   First, the formation rule:

%%% Line 308
\begin{equation}
\dfrac{A \type \qquad B \type}{(A \to B) \type}
\end{equation}

%%% Line 311
Next, the introduction rule, so that we can construct functions:

%%% Line 313
\begin{equation}
\dfrac{x : A \vdash b : B}{ \lambda x.b : A \to B}
\end{equation}

%%% Line 316
The elimination rule shows that terms of a function type
behave like functions, namely, there is a notion of 
evaluation of function on arguments:

%%% Line 320
\begin{equation}
\dfrac{f : A \to B \qquad a: A}{ f(a) : B}
\end{equation}

%%% Line 324
Finally, the computation rule:

%%% Line 327
\begin{equation}
\dfrac{x : A \vdash b : B \qquad a : A}{(\lambda x.b) a = b[a/x]: B}
\end{equation}

%%% Line 332
The elimination and computation rule taken together tell us how 
to define functions out of \(A\).

%%% Line 335
\subsection{Functions of Several Variables} \label{functions-of-several-variables}

%%% Line 337
All functions in MLTT are functions of one variable.   However, because functions can return functions as values, we can 
in effect work with functions that take more than one argument. 
To see how this works, consider three types \(A\), \(B\), and \(C\).
Using the formation rule for functions we construct the
type of functions \(B \to C\).   Since this is a type, we can
apply the formation rule to create the type \(A \to (B \to C)\).
By convention, type formation is right associative, so we usually 
write this as \(A \to B \to C\).   If what we want is \((A \to B) \to C\), then the parentheses are mandatory.

%%% Line 346
What can we do with a term \(f\) of type \(A \to B \to C\)? Suppose given terms \(a : A\) and \(b : B\).   Because \(f : A \to (B \to C)\), 
the elimination rule tells us that \(f(a) : B \to C\).   Therefore 
we can apply \(f(a)\) to \(b\), obtaining \(f(a)(b) : C\). This is how we evaluate \(f\).   We say that \(f(a)\), which is a function, is the result of \index{partial evaluation}\textit{partial evaluation}.

%%% Line 350
In functional programming languages, it is common to write \(f\, a\) instead of \(f(a)\).   Thus, instaead of writing \(f(a)(b)\), we could write   \((f\, a)\, b\).   Here another convention comes into play: evaluation of functions is left-associative, so we may write \(f\, a\, b\) without ambiguity.

%%% Line 354
\section{The Boolean Type} \label{the-boolean-type}

%%% Line 356
The Boolean type has two constructors and just two terms:

%%% Line 358
\begin{equation}
\dfrac{}{\boolean \type}\qquad\dfrac{}{\true : \boolean}\qquad\dfrac{}{\false : \boolean}
\end{equation}

%%% Line 365
To define a function \(f : \boolean \to C\), we need the
elimination and computations rules. Below is the 
elimination rule.   It defines a function called the \index{recursor}\textit{recursor}.
It is a kind of universal function-builder: given certain data as 
inputs, it produces a function \(f : \boolean \to C\) as output, namely 

%%% Line 371
\begin{equation}
f\, b = \rec_\boolean(t, f, b)
\end{equation}

%%% Line 374
\begin{equation}
\dfrac{C \type \quad t : C \quad f : C \quad b : \boolean}{\rec_\boolean(t, f, b): C}
\end{equation}

%%% Line 378
To complete the picture, we must give the computation rules:

%%% Line 380
\begin{equation}
\dfrac{t : C\qquad f : C}{{\rec_\boolean(t, f, \true) = t : C}}
\end{equation}

%%% Line 384
\begin{equation}
\dfrac{t : C\qquad f : C}{{\rec_\boolean(t, f, \false) = f : C}}
\end{equation}

%%% Line 388
The first computation rule says that if the last argument of \(\rec\)
is \(\true\), the return value is \(t: C\).   If it is false, the return value is \(f : C\).   Let us use what we have learned to define the function \(\nott : \boolean \to \boolean\) which negates its
argument:

%%% Line 392
\begin{equation}
\nott\, b = \rec_{\boolean}(\false,\true, b)
\end{equation}

%%% Line 395
Exercise: show that \(\nott \true ≡ \false\) and \(\nott \false ≡ \true\).

%%% Line 397
\textbf{Pattern matching.} We can also define \(\nott\) by pattern-matching:

%%% Line 399
\begin{align}
&\nott : \boolean \to \boolean\\
&\nott \true =\false\\
&\nott \false = \true
\end{align}

%%% Line 404
It is a mechanical procedure to translate pattern-matching
definitions into definition by eliminator.

%%% Line 407
\textbf{Boolean algebra}

%%% Line 409
Let's see if we can implement a fragment of the standard operations
in Boolean algebra. Below is how we might implement logical conjunction.   Here is one way to do it, using pattern-matching:

%%% Line 412
\begin{align}
&\and : \boolean \to \boolean \to \boolean\\
&\and \true \true = \true\\
&\and \true \false = \false\\
&\and \false \true= \false\\
&\and \false \false = \true
\end{align}

%%% Line 419
This style of pattern-matching can be improved:

%%% Line 421
\begin{align}
\label{\and-def}\\
&\and \, - \false = \false\\
&\and x \true = x
\end{align}

%%% Line 426


%%% Line 430
Here the first argument (underscore) of the first equation means 
\quote{ignore this value.}

%%% Line 433
As before, the function in question can also be defined
via the recursor:

%%% Line 436
\begin{equation}
\label{\rec-def-of-adn}\and = \lambda b_1 . \lambda b_2 . \rec_\boolean(b_2, \false, b_1)
\end{equation}

%%% Line 440
\begin{exercise}
Verify \eqref{rec-def-of-adn} the equations \eqref{and-def}.
\end{exercise}

%%% Line 443
A careful comparison of \eqref{rec-def-of-adn} and \eqref{and-def}
will show that the two derivations are interderviable.

%%% Line 446
\begin{exercise}
Define the logical function \(\or\) by (1) equations, (2) the eliminator.
\end{exercise}

%%% Line 449
Assuming the previous exercise, consider the following
sequence of reductions

%%% Line 452
\begin{align}
\label{bool-reductions}\\
&\or\ (\and\ (\or\ \true\ \false)\ \true)\ \false \to\\
&\or\ (\and\ \true\ \true)\ \false \to\\
&\or\ (\true\ \false) \to\\
&\true
\end{align}

%%% Line 459
Each line \eqref{bool-reductions} is a term of \(\boolean\).   However,
the last term is special: it cannot be further reduced.   Such a term is said to be in \index{normal form}\textit{normal form}.   Moreover, one can assert judgments like the below.   See the item (5) in section \ref{mltt-as-a-formal-system}.

%%% Line 462
\begin{equation}
\label{\boolean-computation}\or\ (\and\ (\or\ \true\ \false)\ \true)\ \false \equiv \true : \boolean
\end{equation}

%%% Line 466
The equality here, as noted in section \ref{mltt-as-a-formal-system},
is \u{definitional}. Note that in this case, computation of the boolean value on the left-hand-side of \eqref{boolean-computation}
is carreid out by a sequence of reductions to normal form.   

%%% Line 475
\section{Comparing Sets and Types} \label{comparing-sets-and-types}

%%% Line 477
Now that we have developed the rudiments of type theory,
let us compare it with set theory.   First,
in set theory, elements exist independently of any sets
into which they me gathered. Indeed, they exist
\emph{prior} to the creation of any set into which they are gathered.   Consider,for example, the natural 
numbers \(\set{}\), \(\set{\set\,}\).   They exist prior to their being gathered into the set of natural numbers \(\nat\).   The situation in type
theory is quite different.   One defines a type, then goes about
constructing terms, adding judgments to \(\mathcal{J}\) as time
proceeds. The type first, then the terms.

%%% Line 487
In set theory a thing like the number \(3\) can be a member of 
more than one set, e.g., the set of 
natural numbrers and the set of prime numbers.   In MLTT, a term
is forever tied to the term from hhisch it came.   It is the result
of application of constructors of that type and no other type,

%%% Line 499
\subsection{Russell's Paradox} \label{russells-paradox}

%%% Line 501
In 1902, Bertrand Russell wrote a letter to Gotlob Frege pointing out an error in Frege's work on the foundations of logic an mathematics.   Russell's letter introduced the set \(R = \sett{ x }{x \not\in x}\)
considered in section \ref{sets}.   The resulting problem --- \index{Russell's paradox}\textit{Russell's paradox} --- meant that there was a flaw in Frege's work on the foundations
of logic and mathematics
   The flaw arises from unrestricted self-reference.   To fix it, Russell devised a theory with
a hierarchy of types in which sets live.   It turns out that an early form of Martin-Löf type   theory
sufferred from a similar paradox due to Girard.   To resolve it, 
Martin-Löf introdcued the hierarchy of universes described in the
next section. I

%%% Line 510
The core idea of Russell's theory is that expressions in logic (and mathematics) are organized into a hierarchy of types:

\begin{itemize}

%%% Line 512
\item  Type 0: Individuals (basic objects)

%%% Line 514
\item Type 1: Sets (or predicates) of individuals

%%% Line 516
\item Type 2: Sets of sets of individuals
%%% Line 518
          And so on…

\end{itemize}

%%% Line 520
An object of a given type can only refer to or contain objects of lower types, never itself or those of the same or higher type.

%%% Line 523
\subsection{Universes} \label{universes}

%%% Line 526
A universe \(\cU\) is a type: one has 

%%% Line 528
\begin{equation}
\cU \type
\end{equation}

%%% Line 532
The terms of \(\cU\) are themselves types. If
\(A : \cU\), then \(A \type\). This setup 
lets you quantify over types safely.
To avoid self-reference and paradoxes, MLTT uses a cumulative hierarchy of universes:

%%% Line 537
\begin{equation}
\cU_0 : \cU_1 : \cU_2 : \ldots
\end{equation}

%%% Line 541
Each universe \(\cU_n\) is a term of the next universe \(\cU_{n+1}\).
 Types in \(\cU_n\) are also in \(\cU_{n+1}\), hence the hierarchy is 
cumulative. This stratification is analogous to Russell's type hierarchy: you can't have a universe that contains itself.

%%% Line 545
We say that \(\cU_n\) is a universe of \index{level}\textit{level} \(n\) and that
\(\cU_0\) is the \index{universe of small types}\textit{universe of small types}. The type of natural numbers
is small, as is the type of functions \(\nat \to \nat\). As we shall see below, there are \quote{large} types.

%%% Line 549
Having introduced universes, we must now modify the rules
of inference which goven types to take into account this new part of the theory.   Here is what we do for the formation and introduction 
rules for the natural numbers and for the construction of function types:

%%% Line 553
\begin{equation}
\label{U-formation}\dfrac{}{\nat : \cU_0}\qquad\dfrac{A : \cU_n \qquad B : \cU_n}{A \to B : \cU_n}
\end{equation}

%%% Line 559
Note that in the case of the function type, the type constructor \(\to\) preserves universe levels: If \(A\) and \(B\) are terms
of \(\cU_n\), then so is \(A \to B\). 

%%% Line 563
\subsection{Large Types} \label{large-types}

%%% Line 565
Consider a function \(f : A \to \cU_0\) where \(A : \cU_0\).   A first 
draft of its formaton rule is

%%% Line 568
\begin{equation}
\label{draft-large-\type}\dfrac{A: \cU_0 \qquad \cU_0:\cU_1}{A \to \cU_0 : \,??}
\end{equation}

%%% Line 572
We write \(B : \, ??\)   because the draft rule doe not fit the form
specified above.   But cumulativity saves the day: \(\cU_0 : \cU_1\)
and if \(A : \cU_0\), so \(A :\cU_1\).   Then \eqref{draft-large-type}
can be written as

%%% Line 577
\begin{equation}
\dfrac{A: \cU_1 \qquad \cU_0:\cU_1}{A \to \cU_0 : \,\cU_1}
\end{equation}

%%% Line 580
We conclude that \(f\) is a term of a type that lives in \(\cU_1\),
namely \(A \to \cU_0\).   Note that in this case the construction
still preserves type levels.

%%% Line 584
\begin{box}
Jumping ahead a bit, the \textbf{\index{Identiy type}\textit{Identiy type}} is formed via 
the rule
%%% Line 588
\begin{equation}
\dfrac{a : A \quad b : A}{ a =_A b}
\end{equation}
%%% Line 591
   The introduction rule is 
%%% Line 593
\begin{equation}
\dfrac{a : A}{\refl_a : a =_A a}
\end{equation}
%%% Line 596
   We say that \(a\) and \(b\) are \textbf{\index{propositionally equal}\textit{propositionally equal}}
if \(a =_A b\) is \textbf{\index{inhabited}\textit{inhabited}}, meaning that there
is a term \(t : a =_A b\).   If \(a\) and \(b\) are definitionally
equal, i.e., if they have the same normal form, then 
\(a \equiv b\).   We conclude that \(a\) and \(b\) are equal types:
\(a =_A b \equiv a =_A a\).   The latter type is inhabited by
\(\refl_a\).   Therefore the former type is inhabited.   Therefore
\(a\) and \(b\) are propositionally equal.
Now consider the function \(f : \nat \to \cU_0\) given 
by \(f\, n :\equiv (0 =_\nat n)\).   Then \(f\) is a term 
in the large type \(\nat \to \cU_0\).
\par\par\par\par
\end{box}

%%% Line 612
\section{Primitive Recursion} \label{primitive-recursion}

%%% Line 615
We will now show that the natural pattern matching equations
for a function like the factorial are defined by a general
scheme called \index{primitive recursion}\textit{primitive recursion}.   Such functions
are guaranteed to terminate on all well-typed inputs.
Once we have the primitive recursion scheme in hand, we will
tease out of it the elimination rule for the natural numbers.
Consider now the 
factorial function:

%%% Line 625
\begin{align}
&\fact : \nat \to \nat\\
&\fact \zero = \suc \zero\\
&\fact\; (\suc n) = (\suc n) * (\fact n)
\end{align}

%%% Line 630
We notice that there two defining equations, one for each
constructor of \(\nat\).   The second equation defines   \(\fact\)
in terms of itself.   However, the definition is not
circular becuase we notice, going in more detail, that
\(\fact\; (\suc n)\) is defined in terms of \(\fact n\). The
argument of \(\fact\) on the right is less than the argument
of \(\fact\) on the right.   Because there is a quantity which
decreases on each function call, the recursion is guaranteed to terminate after finitely many steps.   This is a feature of
of all functions that are definable in MLTT: computations 
always terminate.

%%% Line 641
Here is a sample computation:

%%% Line 643
\begin{align}
&\fact 3\\
&\fact\; (\suc\ 2)\\
&(\suc 2) * \fact 2\\
&3 * \fact\; (\suc 1)\\
&3 * (\suc 1) * \fact 1\\
&3 * 2 * \fact\; (\suc 0)\\
&3 * 2 * (\suc 0)  * (\fact 0)\\
&3 * 2 * 1 * 1
\end{align}

%%% Line 653
Let us now discover the general scheme via which this sort
of recursion works. We look once again at the definition:

%%% Line 656
\begin{align}
&\fact : \nat \to \nat\\
&\fact \zero = \suc \zero\\
&\fact\; (\suc n) = (\suc n) * (\fact n)
\end{align}

%%% Line 661
The first equation has the general form

%%% Line 663
\begin{equation}
f \zero = \base
\end{equation}

%%% Line 666
The second has the form

%%% Line 668
\begin{equation}
f (\suc n) = \step\; n\; (f\; n)
\end{equation}

%%% Line 671
where in this case 

%%% Line 673
\begin{equation}
\step n\; v = (\suc n) * v
\end{equation}

%%% Line 676
Notice that the type of \(\step\) is \(\nat \to \nat \to \nat\).
The recursion scheme for \(\fact\) is the general recursion
scheme displayed below, where \(C = \nat\):

%%% Line 680
\begin{box}

%%% Line 682
   Functions \(f : \nat \to C\), are defined by the equations
%%% Line 684
\begin{align}
&f \zero = \base\\
&f (\suc n) = \step n\; (f\; n)
\end{align}
%%% Line 688
   where
%%% Line 690
\begin{align}
& \base : C\\
& \step : \nat \to C \to C
\end{align}
\end{box}

%%% Line 694
This scheme is what is called \index{primitive recursion}\textit{primitive recursion}. Functions
defined by this scheme terminate on any well-typed argument.

%%% Line 702
\subsection{The Non-Dependent Eliminator} \label{the-non-dependent-eliminator}

%%% Line 704
From our primitive recursion scheme as a guide, we can formulate
the elimination rule for the natural numbers.   We do this here for
non-dependent types.   Consider the problem of defining a function \(f : \nat \to C \to C\).   The data required to do this are

%%% Line 708
\begin{indent}
(i) a type \(C : \cU\)
\par\par
(ii) a term \(\base : C\)
\par\par
(iii) a function \(\step : \nat \to C \to C\)
\end{indent}

%%% Line 715
Let us make up a type for a function \(\rec_\nat\) --- the recursor ---
which takes the above data as input and produces a term of 
\(\nat \to C\) as output.   This straightforward:

%%% Line 719
\begin{equation}
\rec_\nat : (C: \cU) \to C \to (\nat \to C \to C) \to (\nat \to C)
\end{equation}

%%% Line 722
Then 

%%% Line 724
\begin{equation}
f\; n = \rec_\nat\; C\; \base\;  \step\;  n
\end{equation}

%%% Line 727
The computation rules are

%%% Line 729
\begin{align}
&\rec_\nat\; C\; \base\;  \step\;  0 = \base\\
&\rec_\nat\; C\; \base\;  \step\;  (\suc\; n) = \step\; n\; (\rec_\nat\; C\; \base\;  \step\;  n)
\end{align}

%%% Line 733
The factorial function is then defined as

%%% Line 735
\begin{equation}
\fact = \rec_\nat\; \nat\; \zero\; (\lambda\; n\; v \to (\suc n) * v)
\end{equation}

%%% Line 738
\subsection{Addition via the Eliminator} \label{addition-via-the-eliminator}

%%% Line 740
Let us see whether our scheme works for addition.   We recall the definition:

%%% Line 743
\begin{align}
&\add : \nat \to \nat \to \nat\\
&\add \zero n = n\\
&\add\; (\suc m)\; n = \suc\; (\add m \;n)
\end{align}

%%% Line 748
The type of \(\add\) is \(\nat \to \nat \to \nat\), which we
parenthesize as \(\nat \to (\nat \to \nat)\) and then
write as \(\nat \to C\) where \(C = \nat \to \nat\).   Notice that the value 
of \(\add m\) is of type \(C\), and that \(\add 0\) is the identity function.
Of the three pieces of data we need to for the recursor we have two:

%%% Line 754
\begin{equation}
\add = \rec_\nat\; C\; (\lambda n.n)\;\; ??
\end{equation}

%%% Line 757
where \(??\) is the function \(\step\).   Let us now find that function, keeping in mind that \(\step\) maps \(\add\; m\) as a function of \(n\) to 
\(\add\; (\suc\; m)\) as a function of \(n\):

%%% Line 760
\begin{equation}
\step n\; (f\; n) = f\; (\suc\; n)
\end{equation}

%%% Line 766
To do this, we write
the recursion equation defining \(\add\), then abstract with respect to 
the variable \(n\) to get an equality of functions:

%%% Line 770
\begin{align}
&\add\; (\suc m)\; n = \suc\; (\add\; m\; n)\\
&\lambda n.(\add\; (\suc m)\;
\end{align}

%%% Line 774
n) = \lambda n.(\suc\; (\add\; m\; n))

%%% Line 776
Now

%%% Line 778
\begin{align}
\step - (\lambda n.(\add\; m\; n)) &= \lambda n.(\add\; (\suc m)\; n)\\
&= \lambda n.(\suc\; (\add\; m\; n))\\
\step - (g\; n) &=  \lambda n.(\suc\; ( g \; n))
\end{align}

%%% Line 783
\subsection{Pattern-matching vs the Eliminator} \label{pattern-matching-vs-the-eliminator}

%%% Line 785
\section{Propositions as Types} \label{propositions-as-types}

%%% Line 787
Type theory defines its own logic via the doctrine of Propositions as Types:

\begin{itemize}

%%% Line 789
\item  \(\text{Propositions } P \longleftrightarrow \text{Types } P\)

%%% Line 791
\item \(\text{Proofs } p \text{ of } P \longleftrightarrow \text{Terms } p : P\)

%%% Line 793
\item \(\text{Falsehood} \longleftrightarrow \text{ Empty \type}\; \bot\)

\end{itemize}

%%% Line 795
The operations on propositions correspond to type forming operations, e.g.,

\begin{itemize}

%%% Line 797
\item Implication \(P \implies Q \longleftrightarrow A \to B\).

%%% Line 800
\item Negation \(\neg P \longleftrightarrow (P \to \bot)\)

\end{itemize}

%%% Line 803
\subsection{Propositional Logic} \label{propositional-logic}

%%% Line 805
Modus Ponens, one of the principles of classical logic, states that if
we know \(P\) and also \(P \implies Q\), then we know \(Q\).   This has an easy proof in MLTT. If we know \(P\), there is a witness \(p : P\).   If we know
\(P \implies Q\), there is a witness \(f : P \to Q\).   By the elimination rule for functions, \(f\; p : Q\).   Therefore we know \(Q\). \qed{}

%%% Line 809
The principle of the contrapositive is another part of classical logic which
has an easy formulation and proof in MLTT.   The principle states that if \(P \implies Q\), then \(\neg Q \implies \neg P\). For the proof, supoose that \(f : P \to Q\) and \(nq : Q \to \bot\). Then \(nq \circ f : P \to \bot\). \qed{}

%%% Line 813
\subsection{Currry-Howard Correspondence} \label{currry-howard-correspondence}

%%% Line 815
The sketch of propositional logic just given
illustrates the promise that the doctrine of 
Propositions as Types holds.   The Curry-Howard
correspondence, set forth in the table
below, shows how this doctrine suffics
to translate all the notions of the predicate
calculus into type theory.

%%% Line 826
error in constructing table

%%% Line 837
The logic thus defined is a \index{constructive logic}\textit{constructive logic}. Such logics
are built on the notion of proof rather
than truth, as in classical logic.   As a result, the Law of the Excluded Middle (LEM) does not hold, and negation must be
handled with greater care.

%%% Line 842
In this section we discuss salient differences between classical logic and
the constructive logic of MLTT.   In the succeeding sections we fill out the parts
of the Curry-Howard Correspondence that we have not yet treated.

%%% Line 848
\subsection{Negation} \label{negation}

%%% Line 850
We have already seen that the implication 
\((P \to Q) \to (\neg Q \to \neg P)\) holds in MLTT.
In this section we look more closely at negation in MLTT
as compare to classical logic.   The first point is that 
while \(A \to \neg\neg A\) holds in MLTT for all \(A\), the 
principle \(\neg\neg A \to A\) does not.   Thus \(\neg\neg A\) 
and \(A\) are not logically equivalent, as they are in classical logic.

%%% Line 858
\begin{lemma}
\label{lemma-double-neg}
In classical logic, LEM implies both the weak and strong
forms of the Law of Double Negation, \(A \to \neg\neg A\) 
and \(\neg\neg A \to A\), respectively.
\end{lemma}

%%% Line 865
\textbf{Proof.}
\par\par
1. (Strong form). Assume \(\neg \neg A\) is true.   Then \(\neg A\) is false.
By LEM, either \(A\) or \(\neg A\) is true. Since \(\neg A\)
is false, LEM demands that \(A\) be true.   We have proved that \(\neg\neg A \to A\)/
\par\par
2. (Weak form). Assume \(A\) is true.   Then \(\neg A\) is false.   In LEM, 
replace \(A\) by \(\neg A\) to conclude that \(\neg A \lor \neg \neg A\) is true.
Since \(\neg A\) is false, \(\neg \neg A\) must be true.
We have proved that \(A \to \neg\neg A\).
\par\par
\qed{}

%%% Line 879
The type-theoretical translation of Lemma \ref{lemma-double-neg}
states that the types representing 
both strong and weak double negation are inhabited. The argument
below shows that weak double negation holds in MLTT. The strong
form does not, as we argue later in this section 
\ref{law-of-the-excluded-middle} and \ref{heyting-theorem}

%%% Line 886
\begin{proposition}
In MLTT, the Weak Law of Double Negation holds.   That is, 
\(A \to \neg \neg A\) is inhabited.
\end{proposition}

%%% Line 891
\textbf{Proof.} The goal is to construct a term of 

%%% Line 893
\begin{equation}
A \to (A \to \emptyt) \to \emptyt
\end{equation}

%%% Line 896
\textit{First Proof:} Given \(a : A\) and \(f : A \to \emptyt\), we have \(f(a) : \emptyt\). Thus we
have a term of type \(A \to (A \to \emptyt) \to \emptyt\), as required. (2) Alternatively, consider
the \(\lambda\)-term \(t = \lambda (a:A).\lambda (f:A \to \emptyt).f\,a\). Then \(t : A \to (A \to \emptyt) \to \emptyt\),
as required.
\par\par
Q.E.D.
Below is the proof in Agda

%%% Line 906
\begin{verbatim}
dnn : \{A : Set\} → A → ¬ (¬ A)
dnn  a f = f a
\end{verbatim}

%%% Line 910
The code makes more sense if we expand the declaration:

%%% Line 912
\begin{verbatim}
dnn : \{A : Set\} → A → (A → ⊥) → ⊥
dnn  a f = f a
\end{verbatim}

%%% Line 917
\section{Identity Type} \label{identity-type}

%%% Line 919
Consider for moment these two assertions, both 
definitional equalities.

%%% Line 922
\begin{indent}
(i) \(\zero + n \equiv n\)
\par\par
(ii) \(n + \zero \equiv n\)
\end{indent}

%%% Line 928
The first is true: the normal form the left-hand side is \(\zero\)
as is the normal form of the right-hand side.   The second is false, because
if \(n\) is not \(\zero\), there are no rules to reduce the left-hand side.
It is because of (ii) that we need a more powerful form of equalit.   This
is given to us by the \index{identity type}\textit{identity type}:

%%% Line 934
\begin{equation}
\frac{A: \cU \quad a : A \quad b :A}{a =_A b: \cU}
\end{equation}

%%% Line 937
The identity type is a \index{dependent}\textit{dependent} type: its definition depends
on terms of other types.   Notice that formation of the identity types
is an operation within the given universe.      Constructors are given only for the type \(a = a\), as in the equation below. 

%%% Line 941
\begin{equation}
\frac{A : \cU \qquad a : A}{\refl_a : a = a}
\end{equation}

%%% Line 944
If the identity type \(a = b\) is \index{inhabited}\textit{inhabited}, that is, if thre is a term \(p : a = b\) then we say that \(a\) and \(b\) are \index{propositionally equal}\textit{propositionally equal}.   We claim, and will soon prove, that proposiional equality is 
an equivalance relation.   It is, moreover, a \index{congruence}\textit{congruence}: if \(f : A \to B\), there is a function \(\congg f : x =_A y \to f\; x =_B f\;   y\).   Thus, if \(p : x =_A y\), then \(\congg f\;      p : f\; x =_B f\;   y\),

%%% Line 947
Suppose that \(a\) and \(b\) are defiinitionally equal: \(a \equiv b\).
The the types \(a = b\) and \(a = a\) are definitionally equal. The latter type is inhabited by \(\refl_a\). The former type, which is equal to \(a = a\),
is therefore also inhabited.   We have just proved that 

%%% Line 951
\begin{indent}
\textbf{Principle:} \textit{If\(a\) and \(b\) are definitionally equal, then they are also propositionally equal.}
\end{indent}

%%% Line 954
Consider now the type family \(\isRightIdentity n = n + \zero =_\nat n\). 
We aim to show that there is a function \(\lemma : (n : \nat) \to \isRightIdentity n\).   In other words, we claim that the type 
\((n : \nat) \to \isRightIdentity n\) is inhabited.   If that is so, we have proved that 

%%% Line 958
\begin{equation}
\forall n : \nat, n + 0 \text{ is propositionaly equal \to } n
\end{equation}

%%% Line 961
The proof is by induction.   If \(n = 0\), then \(\lemma n = 0 + 0 =_\nat 0\)
is inhabited by \(\refl_0\).   Assume as inductive hypothesis that
\(\lemma n\) is inhabited by \(p\). Then we have

%%% Line 965
\begin{align}
\congg \suc (n + 0 =_\nat 0) &= \suc\; (n + 0) =_\nat \suc n\\
&= (\suc n) + 0 =_\nat = \suc n\\
&= \lemma\; (\suc n)
\end{align}

%%% Line 970
In other words, we have shown that

%%% Line 972
\begin{equation}
\congg\; \suc\; (\lemma n) = \lemma\; (\suc n)
\end{equation}

%%% Line 975
Consequently \(\lemma n\) is proved for all \(n\).   

%%% Line 977
There is one more
bit of information that we can extract from this argument:

%%% Line 980
\begin{indent}
\textit{The term which inhabits\(\lemma n\) is \((\congg \suc)^n \refl_0\)}
\end{indent}

%%% Line 983
As a result, we know not only that \(n + 0\) 
is propositionally equal to \(n\), but
we have in hand a proof term for this fact.   Notice that the proof term
is not \(\refl_x\) for some \(x\), but rather
some function applied to \(\refl_0\).   That is why
both of the following hold: (1) \quote{the identity type is freely generated by\(\refl\)} and (2) \(n + 0\) is propositionally equal to \(n\)
but not definitionally equal. Think of the identity type as being
like a cyclic module \(M\) over a ring \(R\) --- that is a module with one generator \(m\).   An arbitrary
module element has the form \(rm\).   There are as many elements
of this cyclic module as there are elements of the ring \(R\).

%%% Line 996
\subsection{Induction Principle} \label{induction-principle}

%%% Line 999
Consider once again the congruence principle.   It is the function

%%% Line 1001
\begin{equation}
\congg : (f : A \to B) \to (x\; y : A) \to (p : x =_A y) \to f\; x =_B f\; y
\end{equation}

%%% Line 1004
If we apply \(\congg\) to \(f\), we have

%%% Line 1006
\begin{equation}
\congg f : (x\; y : A) \to (p : x =_A y) \to f\; x =_B f\; y
\end{equation}

%%% Line 1010
This function has the form

%%% Line 1012
\begin{equation}
\congg f : (x\; y : A) \to (p : x =_A y) \to C\; x\; y\; p
\end{equation}

%%% Line 1015
where the last type is

%%% Line 1017
\begin{equation}
C\; x\; y\; p :\equiv f\; x =_B f\; y
\end{equation}

%%% Line 1020
In this case there is no dependence on \(p\). Thus constructing the function \(\congg\) is a special case of constructing functions

%%% Line 1022
\begin{equation}
f : (x, y: A) \to (p : x =_A y) \to C(x, y, p)
\end{equation}

%%% Line 1025
where \(C(x,y,p)\) is a family of types depending on \(x, y : A\)
and \(p: x =_A y\). Proofs for symmetry and transitivity requires
exactly the same construction. To construct the output \(f\), we need certain inputs:

\begin{enumerate}

%%% Line 1029
\item The type family \(C : (x, y : A) \to (x =_A y) \to \cU\)

%%% Line 1031
\item A function \(c : (x : A) \to C(x,x,\refl_x)\)

\end{enumerate}

%%% Line 1033
The type family is the one appearing in the type of \(f\), so the 
new element is the function \(c\).   It assigns to 
\(\refl_x : x =_A x\) a term of type \(C(x,x,\refl_x)\).
That is all the data needed to construct \(f\).
The induction principle for the identity type states that given
these inputs, there is a function \(f\) of the type indicated above which satisfies

%%% Line 1040
\begin{equation}
f(x,x,\refl_x) :\equiv c(x)
\end{equation}

%%% Line 1043
\subsection{Congruence} \label{congruence}

%%% Line 1045
Consider the hypothetical function \(\congg f\):

%%% Line 1047
\begin{equation}
\congg f : (x\; y : A) \to (p : x =_A y) \to f\; x =_B f\; y
\end{equation}

%%% Line 1051
Let us construct it using the induction principle.   For \(C\), 
we choose the family \(C\; x\; y = f\; x =_B f\; y\) and we set

%%% Line 1054
\begin{equation}
c\; x = \refl_{(f\, x)} : C\; x\; x
\end{equation}

%%% Line 1057
This makes sense because \(C\; x\; x = (f\; x =_B f\; x)\).

%%% Line 1059
\textbf{Alternate method of proof.} There is an equivalent, alternate method of proof which goes as follows.      To construct

%%% Line 1061
\begin{equation}
\congg f : (x\; y : A) \to (p : x =_A y) \to f\; x =_B f\; y
\end{equation}

%%% Line 1064
it suffices to consider the case where \(x\) and \(y\) are the same
and \(p = \refl_x\).   Then we have

%%% Line 1067
\begin{equation}
\congg f : (x\; x : A) \to (p : x =_A x) \to f\; x =_B f\; x
\end{equation}

%%% Line 1070
It suffices to set 

%%% Line 1072
\begin{equation}
\congg f : x\; \refl_x = \refl_{(f x)}
\end{equation}

%%% Line 1076
\subsection{Symmetry} \label{symmetry}

%%% Line 1078
Let us see how the more informal method works to prove symmetry.   What we
must prove is that the following function exists: 

%%% Line 1081
\begin{equation}
\sym : (x : A) \to (y : A) \to (x =_A y) \to (y =_A x)
\end{equation}

%%% Line 1084
When \(x\) and \(y\) are the same, this reads

%%% Line 1086
\begin{equation}
\sym : (x : A) \to (x : A) \to (x =_A x) \to (x =_A x)
\end{equation}

%%% Line 1089
We set

%%% Line 1091
\begin{equation}
\sym\;x\; x\; \refl_x = \refl_x
\end{equation}

%%% Line 1094
Then \(\sym\) is defined when \(x = y\) and therefore defined for all \(x\)
and \(y\).

%%% Line 1097
\subsection{Transitivity} \label{transitivity}

%%% Line 1099
\begin{equation}
\trans : (x : A) \to (y : A)\to (z : A)\to (x =_A y) \to (y =_A z) \to (x =_A z)
\end{equation}

%%% Line 1102
Take \(y = x\), \(p = \refl_x\) and substitute, letting
\(f\; z\; q = \trans\, x\; x\; z\; \refl_x q\) where
\(f : (z : A) \to (x =_A z) \to (x =_A z)\). Now take \(z = x\).   Then the type of the second argument of \(f\) is \(x =_A x\), which is inhabited 
by \(\refl_x\).   Set 

%%% Line 1107
\begin{equation}
f\; x \refl_x = \refl_x
\end{equation}

%%% Line 1110
To conclude,

%%% Line 1112
\begin{equation}
\trans\; x\; x\; x\; \refl_x \;\refl_x = \refl_x
\end{equation}

%%% Line 1116
\section{Induction Principle for \(\nat\)} \label{induction-principle-for-nat}

%%% Line 1118
Recall that to define a function \(f : \nat \to C\), where \(C\)
is some type, we use the elimination rule in the form of the recursor:

%%% Line 1122
\begin{equation}
\rec_\nat : (C: \cU) \to C \to (\nat \to C \to C) \to (\nat \to C)
\end{equation}

%%% Line 1125
To construct proofs of assertions about the natural numbers,
we need the elimination rule in its full power, namely where \(C: \nat \to \cU\) is ia type family. The   output of the rule is a dependent function 

%%% Line 1128
\begin{equation}
f : (n : \nat) \to C\; n
\end{equation}

%%% Line 1131
The inputs are

\begin{enumerate}

%%% Line 1133
\item a type family \(C : \nat \to \cU\)

%%% Line 1135
\item a term \(\base: C\; \zero\)

%%% Line 1137
\item a function \(\step : (n : \nat) \to C\; n \to C (\suc\; n\))

\end{enumerate}

%%% Line 1139
The elimination rule for the natural numbers and type families
can be read off this input-output data:

%%% Line 1142
\begin{equation}
\ind_\nat (C : \nat \to \cU) \to C\; \zero \to ((n: \nat) \to C\; n \to C (\suc n)) \to ((n : \nat) \to C)
\end{equation}

%%% Line 1145
The computation rules are

%%% Line 1147
\begin{align}
&\ind_\nat\; C\; \base\, \step\, \zero =  \base\\
&\ind_\nat\; C\; \base\, \step\, (\suc n) =  \step n\; (\ind_\nat\; C\; \base
\end{align}

%%% Line 1151
Unsurprisingly, these are of the same form as the computaton rules for 
the recursor.

%%% Line 1155
\section{Theory and Metatheory} \label{theory-and-metatheory}

%%% Line 1157
\subsection{Normal Forms} \label{normal-forms}

%%% Line 1159
We defined definitional equality in terms of normal forms.   That begs
some foundational questions: (1) do the exist? (2) are they unique?
To see that there is an issue, let us consider the lambda calculus once again.   Let \(\Omega = \lambda x. x\, x\)   Then we have the reduction sequence

%%% Line 1163
\begin{equation}
\Omega \Omega \xrightarrow{\beta}\Omega \Omega \xrightarrow{\beta} \Omega \Omega \xrightarrow{\beta}\ldots
\end{equation}

%%% Line 1168
The reduction sequence never terminates, and so \(\Omega\Omega\) has 
no normal form. 

%%% Line 1171
In MLTT, normal forms consist entirely of constructors.

%%% Line 1173
\subsection{Typability SLTC} \label{typability-sltc}

%%% Line 1175
This and other considerations led Curch to 
formulate the simply-type lambda calculus (SLTC).   The syntax 
rules change slightly: abstractions carry a type annotation.
Instead of \(\lambda x.x\), we write \(\lambda x : T\), where \(T\)
is a type.   In addition, there are two \index{typing rules}\textit{typing rules}:

%%% Line 1181
\begin{equation}
\frac{x : A \vdash e : B}{(\lambda x: A.e): (A \to B)}\qquad\frac{f : A \to B \quad e : A}{f\; e :B}
\end{equation}

%%% Line 1186
The first of these   assings a type to   functions.   The second defines the type of a function application.   Let's take a second look at
\(\Omega = \lambda x.x\,x\).   What is its type? Let us assume that \(x\)
has type \(A\).   Since \(x\,x\) is an application, \(x\) also has type \(X \to Y\) for some \(X\) and \(Y\). Since \(x\) is applied to \(x\), which has type \(A\), we conclude that \(X = A\). Therefore \(A = A \to Y\).   We try to solve this equation for \(A\), the type of \(x\): 

%%% Line 1190
\begin{equation}
A = A \to Y = (A \to Y) \to Y = ((A \to Y) \to Y) \to Y \cdots
\end{equation}

%%% Line 1193
There is no solution, so \(x\) is not typeable and therefore \(\Omega\) is
not typeable.

%%% Line 1196
\subsection{Typeability in Practice} \label{typeability-in-practice}

%%% Line 1198
Consider functions \(\double : \nat \to \nat\) and \(\isEven : \nat \to \boolean\).   Let's ask whether the expressions (1) \(\isEven\; (\double 3)\)
and (2) \(\double\; (\isEven 3)\) are typeable. The tree for typeing the first of these is

%%% Line 1201
\begin{equation}
\dfrac{\nat \to \boolean \qquad \dfrac{\nat \to \nat \quad \nat}{\nat}}{\boolean}
\end{equation}

%%% Line 1204
For the second it is 

%%% Line 1206
\begin{equation}
\dfrac{\nat \to \nat \qquad \dfrac{\nat \to \boolean \quad \nat}{\boolean}}{\text{Stuck!}}
\end{equation}

%%% Line 1209
In the second case, for \(\double\; (\isEven 3)\), we reach a stage in computing the tree of types (from the leaves of the tree to the root)
beyond which we cannot proceed.   We are stuck, and so the expression
in turn is not typeable.

%%% Line 1213
\subsection{Strong normalization} \label{strong-normalization}

%%% Line 1215
We say that a formal system satisfies \index{strong normalization}\textit{strong normalization} if every well-typed term reduces to normal form in finitely many steps. Both MLTT
and SLTC are strongly normalizing.      

%%% Line 1219
\subsection{Confluence} \label{confluence}

%%% Line 1221
Suppose that a term \(t\) has reductions \(t \to\!\!*\,u_1\) and 
\(t \to\!\!*\, u_2\). The symbol \(\to\!\!*\,\) means a sequence of reductions.   Supposer further that there is a term \(v\) and reductions
\(u_1 \to\!\!*\, v\) and \(u_2 \to\!\!*\, v\). If there is such a \(v\) for any 
pair of reductions of \(t\), we say that the system is \index{confluent}\textit{confluent} (or satisfies the Church-Rosser property).   MLTT and SLTC are both confluent.   This means that normal forms are unique 

%%% Line 1227
\subsection{Consistency} \label{consistency}

%%% Line 1229
A system is consistent if it is not possible to derive a contradiction.   By this we mean to derive both a proposition \(P\) and its negation \(\neg P\).
Suppose that this is possible in MLTT. Then we have witnesses \(p : P\)
and \(f : P \to \bot\).   Then \(f\; p\) is a term of \(\bot\).   Therefore, if
MLTT is inconsistent, the empty type is inhabited.   Conversely, if the empty type is inhabited, then all propositions are provable.   This follows from the elimination principle for the empty type.   If all propositons are provable, one may prove both \(P\) and \(\neg P\).

%%% Line 1234
Suppose that there is a witness \(p\) for the empty type.   By strong
normalization, \(p\) reduces to a normal form of \(\bot\).   Normal forms
consist entirely of constructors.   But \(\bot\) has no constructors.
Arguing by contradciton, we conclude that MLTT is consistent.

%%% Line 1239
\textbf{Note.} The argument just given applies to pure MLTT. If one adds
axioms like function extensionality or univalence, as in homotopy type theory, strong normalization is lost.

%%% Line 1243
\subsection{Structural Rules} \label{structural-rules}

%%% Line 1245
In addition the type-specific rules (formation, introduction, elimination,
and computation), MLTT has various structural rules.

\begin{itemize}

%%% Line 1248
\item \textbf{\index{Disjontness.}\textit{Disjontness.}}   If a type has multiple constructors, e.g. \(\zero\) and \(\suc\), their outputs cannot be equal, eg. \(\zero \ne \suc n\) for any \(n\)

%%% Line 1250
\item \textbf{Injectivity of construtors.} If \(c x = c y\), then \(x = y\).   This important for pattern-matching.

\end{itemize}

%%% Line 1252
\subsection{Metatheory} \label{metatheory}

%%% Line 1254
Metatheory is theory about theory.

\begin{itemize}

%%% Line 1256
\item Does every well-typed term   reduce to normal form?

%%% Line 1258
\item Is the type-checking algorithm decideable?   A decideable algorithm determines, in finitely many steps, whether a give term \(t\) has type \(A\) in aa given context \(\Gamma\).   That is, for given \(t\), \(\Gamma\), and \(A\), does \(\Gamma \vdash t : A\) hold?

%%% Line 1262
\item Is substitution admissble (do judgments respect variable replacement).

%%% Line 1264
\item Termination.   In MLTT, termination is enforced by structural recursion or by some well-founded measure that decreases on each function call.

%%% Line 1267
\item Totality.   Consdier the (Haskell) function \(f\; n = \text{if } n == 0 \text{ then } 1 \text{ else } f ( n - 1)\). It is \textbf{\index{total}\textit{total}} over \(\nat\): it terminates and is defined for all \(n : \nat\). Now consider \(g\; n = \text{if } n == 0 \text{ then } 1 \text{ else } g\, n\). This function is not total: it loops forever on input \(n > 0\).   In MLTT,   functions are total by design. The type checker will exclude functions with missing cases or which loop forever.   Thus non-total functions are not definable in Elm.

\end{itemize}

%%% Line 1272
\section{Models and LEM} \label{models-and-lem}

%%% Line 1274
A model \(\cM\) of a formal system \(\cF\) is a structure-preserving 
map \(\nu : \cF \to \cA\) where \(\cA\) is some kind of algebraic object 
such as a Boolean or Heyting algebra.   

\begin{itemize}

%%% Line 1278
\item \textbf{Consistency.} \(\cF\) has at least one model.

%%% Line 1280
\item \textbf{Soundness.} \(\Gamma \vdash \varphi \implies \Gamma \models \varphi\), that is, if \(\varphi\) is derivable, it is true in all models.

%%% Line 1283
\item \textbf{Completeness.}   \(\Gamma \models \varphi \implies \Gamma \vdash \varphi\) that is, if \(\varphi\) is true in all models, it is derivable.

\end{itemize}

%%% Line 1285
Propositional logic is consistent, sound, and complete.

%%% Line 1288
error in constructing table

%%% Line 1295
Each row in the table has a difrent assignment of truht 
values to \(p\) and \(q\).   Each row is a model.   In this case there
are just \(2\) propositional variables \(p\) and \(q\), so there are only \(4\) models.   The column for \(\leftrightarrow\) consists entirely of \(T\)'s, so we can say that \(\models p \land q \leftrightarrow q \land p\) --- this formula is true in all models.

%%% Line 1299
\subsection{Heyting Models and LEM} \label{heyting-models-and-lem}

%%% Line 1301
Let \(\cA\) the Heyting algebra of open sets of a topological space \(X\).   The   operations are union, intersection, and Heyting complement: If \(U\) is an open set, then \(\neg U\) is the interior of the complement of the set-theoretic complement of \(U\).   A Heying model of a propositional calculus \(\cP\) is a map \(\cO : \cP \to \cA\) that sends propositions to open sets
and respects logical connectives on the left and union, intersection, and Heyting complement on the right.   One has \(\cO(T) = X\) and \(\cO(F) = \empty\).   Moreover, axioms map to \(\cO(T)\), and inference rules preserve truth: if the premises map to \(\cO(T)\), then so does the conclusion.

%%% Line 1304
Consider the law of the excluded middle (LEM). It says that for all propositions \(P\), \(P \lor \neg P\) holds.   In such a system, \(\cO(P) \cup \neg \cO(P) = X\).   

%%% Line 1306
Now consider a system like intuitionistic logic in whch LEM is not an
axiom.   We ask: is it derivable in intuitionistic logic..   This is a question that model theory
can answer.   Intuitionistic logic has axioms and inference rules,
and one can set models so that these are preserved. Choose such a model 
so that a distinguised proposition \(P\) maps to the open set \(x > 0\).
Then 

%%% Line 1313
\begin{equation}
\cO (\neg P) \cup (P) = \sett{x \in \bR}{ x < 0} \cup \sett{x \in \bR}{ x >  0} = \sett{x \in \bR}{ x \ne 0}
\end{equation}

%%% Line 1316
Consequently LEM is not derivalble in intuitonistic logic.

%%% Line 1321
\begin{remark}
If the topology of \(X\) is discrete, then the set-theoretic 
complement and the Heyting complement are the same. LEM cannot be
refuted by such models.
\end{remark}

%%% Line 1330
\section{References} \label{references}

%%% Line 1332
\textbf{Models}

%%% Line 1334
\href{https://www2.mathematik.tu-darmstadt.de/~streicher/}{Hoffman and Streicher}

%%% Line 1336
\textbf{Hedberg's Theorem}

%%% Line 1338
\href{https://martinescardo.github.io/GeneralizedHedberg/html/GeneralizedHedberg.html}{Escardo et al, Generalized Hedberg}

%%% Line 1340
\href{https://github.com/agda/agda-stdlib/blob/master/src/Axiom/UniquenessOfIdentityProofs.agda}{UIP@stdlib}

%%% Line 1342
\href{https://www.cs.uoregon.edu/research/summerschool/summer14/rwh_notes/notes_week9.pdf}{Harper's U Oregon Course}

%%% Line 1344
\href{https://www.andrew.cmu.edu/user/awodey/hott/papers/hedberg.pdf}{Hedberg's article}

%%% Line 1346
\href{https://doisinkidney.com/code/probability/Cubical.Relation.Nullary.DecidableEq.html}{essence of the proof using CuTT}

%%% Line 1348
\href{https://www.cs.uoregon.edu/research/summerschool/summer14/rwh_notes/notes_week5.pdf}{U Orego course partial summary}

%%% Line 1350
\href{https://www.cse.chalmers.se/~nad/listings/equality/Equality.Decidable-UIP.html}{chalmers cubical hedberg}

%%% Line 1352
\href{https://math.stackexchange.com/questions/2208376/what-am-i-not-understanding-about-hedbergs-theorem}{stackexchange}

%%% Line 1354
\href{https://planetmath.org/72uniquenessofidentityproofsandhedbergstheorem}{planetmath}

%%% Line 1356
\href{https://scs.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=ac7edbff-ef18-4204-b2d4-d45f58bf3b34}{Harper Lecture}

%%% Line 1358
\href{https://www.math.fsu.edu/~ealdrov/teaching/2020-21/fall/MAS5932/agda/sets-logic.html#hedberg}{FSU notes}

%%% Line 1360
\href{https://martinescardo.github.io/papers/hedberg.pdf}{Escardo etal generalizaitons of Hedberg's theorem}

%%% Line 1363
\href{https://git.mzhang.io/michael/type-theory/src/commit/f10e3a09b9c83d8534466057d40e4fc7e5cde5af/src/HoTTEST/Agda/Lecture-Notes/files/Hedbergs-Theorem.lagda.md}{Zhang cubical Hedberg}

\clearpage

\printindex



\end{document}
