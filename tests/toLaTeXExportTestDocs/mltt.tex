\documentclass[11pt, oneside]{article}



%% Packages

\usepackage{listings}

%% Standard packages
\usepackage{geometry}
\geometry{letterpaper}
\usepackage{changepage}  % for the adjustwidth environment
\usepackage{graphicx}    % for \includegraphics

%% Index, hyperref
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{hyperref}   % load before imakeidx
\usepackage{imakeidx}

%%%%
\usepackage[normalem]{ulem} % for \st
\usepackage{soul}           % for \hl and \sethlcolor
\usepackage{wrapfig}        % for wrapfigure

%% AMS
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{amscd}

\usepackage{fancyvrb} %% for inline verbatim

%% Chemistry
\usepackage[version=4]{mhchem} % for \ce



%% Commands

\newcommand{\hang}[1]{%
  {%
    \setlength{\leftskip}{1em}%
    \setlength{\hangindent}{1em}%
    \hangafter=1 %
    #1\ \vpace{4}%
  }%
}

\renewcommand{\labelitemi}{\scalebox{0.7}{\textbullet}}

% Dot box = 1em, gap = 1em → total = 2em
\newcommand{\compactItem}[1]{%
  \par
\noindent
  \hangindent=2em \hangafter=1%
  \makebox[1em][l]{\labelitemi}\hspace{1em}#1\par
}

\newcommand{\code}[1]{{\tt #1}}
\newcommand{\ellie}[1]{\href{#1}{Link to Ellie}}
% \newcommand{\image}[3]{\includegraphics[width=3cm]{#1}}

%% width=4truein,keepaspectratio]


% imagecentercaptioned command removed - using standard figure environment instead

\newcommand{\imagecenter}[2]{
   \medskip
   \begin{figure}[htp]
   \centering
    \includegraphics[width=#2]{#1}
    \vglue0pt
    \end{figure}
    \medskip
}

\newcommand{\imagefloat}[4]{
    \begin{wrapfigure}{#4}{#2}
    \includegraphics[width=#2]{#1}
    \caption{#3}
    \end{wrapfigure}
}


\newcommand{\imagefloatright}[3]{
    \begin{wrapfigure}{R}{0.30\textwidth}
    \includegraphics[width=0.30\textwidth]{#1}
    \caption{#2}
    \end{wrapfigure}
}

\newcommand{\hide}[1]{}


\newcommand{\imagefloatleft}[3]{
    \begin{wrapfigure}{L}{0.3-\textwidth}
    \includegraphics[width=0.30\textwidth]{#1}
    \caption{#2}
    \end{wrapfigure}
}
% Font style
\newcommand{\italic}[1]{{\sl #1}}
\newcommand{\strong}[1]{{\bf #1}}
\newcommand{\strike}[1]{\st{#1}}

% Scripta
\newcommand{\ilink}[2]{\href{{https://scripta.io/s/#1}}{#2}}
\newcommand{\markwith}[1]{}
\newcommand{\anchor}[1]{#1}

% Color
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\violet}[1]{\textcolor{violet}{#1}}
\newcommand{\highlight}[1]{\hl{#1}}
\newcommand{\note}[2]{\textcolor{blue}{#1}{\hl{#1}}}

% WTF?
\newcommand{\remote}[1]{\textcolor{red}{#1}}
\newcommand{\local}[1]{\textcolor{blue}{#1}}

% Unclassified
\newcommand{\subheading}[1]{{\bf #1}\par}
%\newcommand{\term}[1]{{\index{#1}}}
%\newcommand{\termx}[1]{}
\newcommand{\comment}[1]{}
\newcommand{\innertableofcontents}{}


% Special character
\newcommand{\dollarSign}[0]{{\$}}
\newcommand{\backTick}[0]{\`{}}

%% Theorems
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{axiom}{Axiom}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{problem}{Problem}
\newtheorem{exercises}{Exercises}
\newcommand{\bs}[1]{$\backslash$#1}
\newcommand{\texarg}[1]{\{#1\}}


%% Environments
\renewenvironment{quotation}
  {\begin{adjustwidth}{2cm}{} \footnotesize}
  {\end{adjustwidth}}

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist

\renewenvironment{indent}
  {\begin{adjustwidth}{0.75cm}{}}
  {\end{adjustwidth}}


%% NEWCOMMAND

% \definecolor{mypink1}{rgb}{0.858, 0.188, 0.478}
% \definecolor{mypink2}{RGB}{219, 48, 122}
\newcommand{\fontRGB}[4]{
    \definecolor{mycolor}{RGB}{#1, #2, #3}
    \textcolor{mycolor}{#4}
    }

\newcommand{\highlightRGB}[4]{
    \definecolor{mycolor}{RGB}{#1, #2, #3}
    \sethlcolor{mycolor}
    \hl{#4}
     \sethlcolor{yellow}
    }

\newcommand{\gray}[2]{
\definecolor{mygray}{gray}{#1}
\textcolor{mygray}{#2}
}

\newcommand{\white}[1]{\gray{1}[#1]}
\newcommand{\medgray}[1]{\gray{0.5}[#1]}
\newcommand{\black}[1]{\gray{0}[#1]}

% Spacing
\parindent0pt
\parskip5pt

\makeindex[
                          title=Index,
                          columns=2,
                          %% intoc     % include index in the table of contents
                        ]

\begin{document}

\title{Untitled}

\date{}

\author{
test-author
}

\maketitle

\tableofcontents

%%% Line 6




%%% Line 3


%%% Line 6


%%% Line 49


%%% Line 52
\section{Sets} \label{sets}

%%% Line 54
A set is a collection of elements.   Suppose that we
have   things like this: \(4\heartsuit, 5\diamondsuit, 6\clubsuit, 2\spadesuit\). We can gather them in to various collections, e.g.,

%%% Line 57
\begin{align}
A = \set{ 2\heartsuit, 5\heartsuit}\\
B = \set{2\heartsuit, 7\spades}
\end{align}

%%% Line 61
It makes sense to say \(e \in A\), meaning \quote{the element\(e\) is in the set \(A\)}.   Thus we have 

%%% Line 64
\begin{equation}
2\heartsuit \in A, \ 2\heartsuit\in B,\ 7\spadesuit \not\in A,\ 7\spadesuit \in B
\end{equation}

%%% Line 67
Assertions like \(2\heartsuit \in A\) or \(7\spadesuit \in A\) are \term{propositions}: they are capable of being true or false.   True in the first case, but false in the second.

%%% Line 69
Two important observations: the existence of the elements \(2\heartsuit, 5\heartsuit\), etc. is prior to the existence of the sets \(A\), \(B\), etc.   Also, a thing like \(2\heartsuit\) can be an element of more than one set, e.g., \(A\) and \(B\).

%%% Line 71
\subsection{Encoding things as sets} \label{encoding-things-as-sets}

%%% Line 73
Mathematical objects like the natural numbers can be described in the language of sets.   Here is one way to do   it. Encode zero as
the empty set \(\set{}\).
Encode one as \(\set{\set{}}\).   Encode two as 
\(\set{ \set{}, \set{\set{}}}\), etc. Introduce aliases for these numbers:

%%% Line 78
\begin{align}
&0 :\equiv \set{}\\
&1 :\equiv \set{\set{}}\\
&2 :\equiv \set{ \set{}, \set{\set{}}}\\
&etc.
\end{align}

%%% Line 84
Then we find \(0 \in 1\), \(0 \in 2\) and \(1 \in 2\), etc.   However,
none of these elements is element of itself: \(0 \not\in 0\), \(1 \not\in 1\), etc.

%%% Line 87
We can gather these numbers into sets in various ways: the set of all natural numbers,   \(\nat = \set{0, 1, 2, \ldots}{}\), the subsets of \(\tt{Even}\) and \(\tt{Odd}\) numbers, respectively.   The latter is described using a predicate --- a function like \(\text{isOdd}\) that returns true or false according to whether the argument of the
function is even or odd.

%%% Line 90
\begin{equation}
\tt{Odd} = \sett{x \in \nat}{\text{ isOdd(x)} = \text{\true}}
\end{equation}

%%% Line 93
Now consider the set   whose elments are not members of themselves:

%%% Line 95
\begin{equation}
R = \sett{x}{x \not\in x}
\end{equation}

%%% Line 98
Its definition is vaguely like that of \(\tt{Odd}\).
The numbers \(0, 1, 2, \dots\) are elements of \(R\), as 
are the sets \(\nat\), \(\tt{Odd}\), and \(\tt{Even}\).   But now ask if \(R\) is an element of itself.   If \(R \in R\), we conclude from the definition of \(R\), that \(R \not \in R\).   If \(R \in R\), we conclude
from the definition of \(R\), that \(R \in R\).   In either case, we find a contradiction, and so we are faced with a paradox.   We will revisit this
paradox in section \ref{comparing-sets-and-types}, where we discuss the origins of type theory and introdue the notion of type universes.

%%% Line 106
\section{Formal Systems} \label{formal-systems}

%%% Line 108
Martin-Löf type theory is a formal system, the two main ingredients
of which are \term{judgments} and \term{inference rules}.   Judgments
are the things we assert.   They are the facts known to the system.
Rules of inference are rules use to derive new judgments from
existing ones.   Their general form is 

%%% Line 115
\begin{equation}
\frac{P_1 \ P_2 \ \ldots \ P_n}{C}
\end{equation}

%%% Line 119
where the \(P_i\) are the premises and \(C\) is the conclusion.   If
the premises are judgments, then so is the conclusion. The conclusion of a rule with no premises is a judgment.

%%% Line 123
As an example, we consider a version of Goncharov's Cherry-Banana Calculus.   It is a system with symbols \(\heartsuit\) and \(\spadesuit\) and the following inference rules:

%%% Line 126
\begin{equation}
\frac{}{\heartsuit}a\qquad \frac{X}{\spadesuit X}p\qquad\frac{X\spadesuit \quad \spadesuit Y}{XY}c\qquad  \frac{X}{\heartsuit X \spadesuit}
\end{equation}

%%% Line 133
Here the labels are a = axiom, p = prepend, c = collapse, and 
e = enclose   As it stands, the only judgment in this theory
is \(\heartsuit\). Here is a derivation of the judgment 
\(\heartsuit\heartsuit\heartsuit\):

%%% Line 138
\begin{equation}
\dfrac{\dfrac{\dfrac{}{\heartsuit}a}{\heartsuit\heartsuit\spadesuit}e\qquad\dfrac{\dfrac{}{\heartsuit}a}{\spadesuit\heartsuit}p}{\heartsuit\heartsuit\heartsuit}c
\end{equation}

%%% Line 142
If we include the intermediate judgments in the previous derivation, the judgments of the theory are now 

%%% Line 144
\begin{equation}
J_1, J_2, J_3, \ldots = 
\heartsuit,\ \heartsuit\heartsuit\spadesuit,\ 
\spadesuit\heartsuit,\ \heartsuit\heartsuit\heartsuit
\end{equation}

%%% Line 149
where each \(J_n\) is derived from judgments \(J_m\) with \(m < n\).
Not all strings of symbols are derivable.   The string \(\spadesuit\) is an example.   We make think of the derivable strings as the theorems of the formal system. The sequence \(\mathcal{J} = J_1, J_2, J_3, \ldots\) is the current state of knowledge. Note that \(\mathcal{J}\) is a dynamic entity: it grows as more inference rules are applied.

%%% Line 152
\section{MLTT as a Formal System} \label{mltt-as-a-formal-system}

%%% Line 154
There are five kinds of jugments in MLTT.

\begin{enumerate}

%%% Line 156
\item \(\Gamma \ctx\) --- \(\Gamma\) is a well-formed context, that is, a sequence of variable declarations like \(x_1 : A_1, \ldots x_n : A_n\), where latter types \(A_i\) may depend on earlier variables. Something like \(x: A, y: B\) where \(A\) depends on \(y\) is not well formed, nor is \(A, B, C\). In general, well-formed means \quote{formed according to the rules.}

%%% Line 159
\item \(\Gamma \vdash A\ \type\) --- under the assumptions of \(\Gamma\), \(A\) is a type

%%% Line 161
\item \(\Gamma \vdash t : A\) --- under the assumptions of \(\Gamma\), \(t\) is a term of type \(A\)

%%% Line 163
\item \(\Gamma \vdash A \equiv B\ \type\) --- under the assumptions of \(\Gamma\), \(A\) and \(B\) are equal types

%%% Line 165
\item \(\Gamma \vdash a \equiv b: A\) --- under the assumptions of \(\Gamma\), \(a\) and \(b\) are equal terms of type \(A\). We will see what this means in what follows.

\end{enumerate}

%%% Line 169
In the last two judgment forms, equality is \term{definitional equality} or \term{judgmental equality}, i.e., equality according to the rules of computation and definition of the theory.

%%% Line 171
To define a type, we must give four kinds of inference rules: formation, introduction, elimination, and computation.   We begin with the first two rules for the natural numbers.

%%% Line 173
\begin{equation}
\frac{\Gamma \ctx}{\ \Gamma \vdash  \nat\  }\ f\qquad\frac{\Gamma \ctx}{\Gamma \vdash \zero : \nat}\ i_1\qquad\frac{ \Gamma \vdash k : \nat}{\Gamma \vdash \suc k : \nat}\ i_2
\end{equation}

%%% Line 181
The first rule is the formation rule for the natural numbers.   It does nothing more than announce that \(\nat\) is a type in the context \(\Gamma\).   The second two rules are introduction rules.   The first of these says that \(\zero\) is a term of \(\nat\) in context \(\Gamma\).   
The other says that if \(k\) is a term of \(\nat\) in context \(\Gamma\),
then so is \(\suc k\). 

%%% Line 185
By repeated 
application of the last rule, we produce arbitrarily many terms of \(\nat\): \(\zero\), \(\suc \zero\), \(\suc (\suc \zero)\), etc. The symbols \(\zero\) and \(\suc\) are the \term{constructors} of the natural numbers.

%%% Line 190
\begin{box}
The empty type, written \(\bot\), is a type with no introduction
rules, hence no constructors.   Consequently there are no terms of the empty type.
\end{box}

%%% Line 194
\textbf{Contexts.} There are many cases in which   we
%%% Line 197
\begin{equation}
\label{rules-without-\gamma}\frac{}{\   \nat\  }\ f\qquad\frac{}{\zero : \nat}\ i_1\qquad\qquad\frac{  k : \nat}{ \suc k : \nat}\ i_2
\end{equation}
can work with an empty context, in which case we might write the rules in simplified form as

%%% Line 206
We will frequently do this to cut down on visual complexity.   For more about contexts, see the box at the end of this section.

%%% Line 208
Let us continue with our discussion of the natural numbers.   Just as with the Cherry-Banana Calculus, judgments \(n : \nat\) are given by derivations, e.g.,

%%% Line 210
\begin{equation}
\dfrac{\dfrac{\dfrac{}{\zero : \nat}i_1}{\suc \zero: \nat}i_2}{\suc(\suc \zero) : \nat}i_2
\end{equation}

%%% Line 213
At this point our store of judgments is

%%% Line 215
\begin{equation}
\mathcal{J} = \nat\ \type,\ \zero : \nat,\ \suc\ \zero : \nat,\suc\ (\suc \zero) : \nat
\end{equation}

%%% Line 219
Of course it is cumbersome to work with expressions like 
\(\suc\ (\suc\ \zero)\), so we can make definitions
like \(0 :≡ \zero\), \(1   :≡ \suc \zero\), \(2 :≡ \suc\ (\suc \zero)\), 
etc.   Then it makes perfect sense to say that   \(3 ≡ \suc 2\).
This is a \term{definitional} or \term{judgmental} equality.   Later we will study the far more sophisticated notion of
\term{propositional equality}.

%%% Line 227
\begin{box}
Consider the assertion \(\suc x : \nat\).   This is meaningless without the
   assumption \(x : \nat\).   Thus we write \(x : \nat \vdash \suc \nat\).   
   The context is \(x : \nat\).
\//{}   
Contexts are also needed to enforce theory-wide rules that demand that judgments be stable under \term{substitution} and \term{weakening}, i.e., if a term is well-typed in one context, it remains well-typed when new assumptions are added (weakening), or substitutions of variables are made.
\//{}
To reduce visual clutter, we will often write rules in the 
simplified form \eqref{rules-without-gamma}.   However, always
keep in mind that in these cases there is an invisible empty context. It is sometimes written as \(.\)
\//{}
Weakening and substitution are \textbf{structural rules} of MLTT.   They do not define particular types or terms, but rather describe
how judgments behave when we manipulate the context.   They are essential to making sure that the theory is \textbf{stable} and \textbf{compositional}: that building up more complex terms does not
break existing well-formedness.
\//{}
\//{}
\//{}
\textbf{Substitution.} Let's look at an example, then give the formal               rule.   Suppose that \(3 : \nat\) and 
\(x : \nat \vdash \suc x :\nat\).
Then by the subsitution rule, \(\vdash \suc 3 : \nat\)   
\//{}
\textbf{Substitution rule.}Suppose that \(\Gamma \vdash a : A\) and \(\Gamma, x : A, \Delta \vdash J\).   Then \(\Gamma, \Delta[a/x] \vdash J[a/x]\).   The notation \(Q[a/x]\) 
means \quote{replace every occurrence of\(x\) in \(Q\) by \(a\).}   The
expresssion   \(\Gamma, x : A, \Delta\) is the concatenation of of the contexts \(\Gamma\), \(x : A\), and \(\Delta\).
\//{}
\//{}
\//{}
\textbf{Weakening.} The idea is that if a judgment holds in some context,
then it holds if you add more assumptions, even if they are unused. The rule is that if \(\Gamma \vdash J\) and \(\Gamma\vdash   A   \type\)
then \(\Gamma, x : A \vdash J\). In the rule, we have added the variable \(x\) of type \(A\).   The judgment \(J\) remains valid.
\end{box}

%%% Line 258
\section{Lambda Calculus} \label{lambda-calculus}

%%% Line 260
The lambda calculus is the world's smallest programming language.
Nonetheless, it is Turing complete: it can compute anything
that any other programming language can compute, be it a Turing machine, C, Haskell, or any other.   A valid expresson in the lambda calculus is called a \(\lambda\)-term.   The rules for their formation
are as follow:

\begin{enumerate}

%%% Line 265
\item \textbf{Variable Rule.} There is an infinite list of variables \(x_1, x_2, x_3, \ldots\) These are \(\lambda\)-terms.

%%% Line 267
\item \textbf{Abstraction Rule.} If \(x\) is a variable and \(b\) is a lambda-term, then \(\lambda x.b\) is a \(\lambda\)-term.   We call \(b\) the boddy of the lambfda term.   We will think of abstractions as nameless (anonymous) functions.

%%% Line 271
\item \textbf{Application Rule.}   If \(a\) \(b\) are \(\lambda\)-terms, then so is \(a\, b\).   We will think of an appliation as being function   application

\end{enumerate}

%%% Line 275
To give meaning to the lambda calculus, we define the operation of \term{beta-reduction}.   It defines what happens when an abstraction 
is applied to another \(\lambda\)-term:

%%% Line 278
\begin{equation}
\label{\beta-reduction}(\lambda x.b)a = b[a/x]
\end{equation}

%%% Line 283
where \(b[a/x]\) means that all occurrences of \(a\) in \(b\) are replaced by \(x\).   As an example, we have

%%% Line 285
\begin{equation}
(\lambda x.(x + 1))7 \xrightarrow{\beta} (x + 1)[7/x] = 7 + 1 = 8
\end{equation}

%%% Line 288
\textbf{Examples}

%%% Line 290
\((\lambda x.x)a \xrightarrow{\beta} x[x/a] = a\).   This lambda
expressoion acts as the identity function.

%%% Line 293
\begin{align}
(\lambda x. \lambda y  . x)\, 1\, 2 &= (\lambda x. \lambda y  . x)\, 1\, 2\\
& \xrightarrow{\beta} (\lambda y . 1)\, 2\\
& \xrightarrow{\beta}\, 1
\end{align}

%%% Line 298
\begin{align}
(\lambda x. \lambda y  . y)\, 1\, 2 &= (\lambda x. \lambda y  . x)\, 1\, 2\\
& \xrightarrow{\beta} (\lambda y . y)\, 2\\
& \xrightarrow{\beta}\, 2
\end{align}

%%% Line 305
\section{Function Types} \label{function-types}

%%% Line 307
Let us define the type of functions out of a type \(A\) and into a
type \(B\).   First, the formation rule:

%%% Line 310
\begin{equation}
\dfrac{A \type \qquad B \type}{(A \to B) \type}
\end{equation}

%%% Line 313
Next, the introduction rule, so that we can construct functions:

%%% Line 315
\begin{equation}
\dfrac{x : A \vdash b : B}{ \lambda x.b : A \to B}
\end{equation}

%%% Line 318
The elimination rule shows that terms of a function type
behave like functions, namely, there is a notion of 
evaluation of function on arguments:

%%% Line 322
\begin{equation}
\dfrac{f : A \to B \qquad a: A}{ f(a) : B}
\end{equation}

%%% Line 326
Finally, the computation rule:

%%% Line 329
\begin{equation}
\dfrac{x : A \vdash b : B \qquad a : A}{(\lambda x.b) a = b[a/x]: B}
\end{equation}

%%% Line 334
The elimination and computation rule taken together tell us how 
to define functions out of \(A\).

%%% Line 337
\subsection{Functions of Several Variables} \label{functions-of-several-variables}

%%% Line 339
All functions in MLTT are functions of one variable.   However, because functions can return functions as values, we can 
in effect work with functions that take more than one argument. 
To see how this works, consider three types \(A\), \(B\), and \(C\).
Using the formation rule for functions we construct the
type of functions \(B \to C\).   Since this is a type, we can
apply the formation rule to create the type \(A \to (B \to C)\).
By convention, type formation is right associative, so we usually 
write this as \(A \to B \to C\).   If what we want is \((A \to B) \to C\), then the parentheses are mandatory.

%%% Line 348
What can we do with a term \(f\) of type \(A \to B \to C\)? Suppose given terms \(a : A\) and \(b : B\).   Because \(f : A \to (B \to C)\), 
the elimination rule tells us that \(f(a) : B \to C\).   Therefore 
we can apply \(f(a)\) to \(b\), obtaining \(f(a)(b) : C\). This is how we evaluate \(f\).   We say that \(f(a)\), which is a function, is the result of \term{partial evaluation}.

%%% Line 352
In functional programming languages, it is common to write \(f\, a\) instead of \(f(a)\).   Thus, instaead of writing \(f(a)(b)\), we could write   \((f\, a)\, b\).   Here another convention comes into play: evaluation of functions is left-associative, so we may write \(f\, a\, b\) without ambiguity.

%%% Line 356
\section{The Boolean Type} \label{the-boolean-type}

%%% Line 358
The Boolean type has two constructors and just two terms:

%%% Line 360
\begin{equation}
\dfrac{}{\boolean \type}\qquad\dfrac{}{\true : \boolean}\qquad\dfrac{}{\false : \boolean}
\end{equation}

%%% Line 367
To define a function \(f : \boolean \to C\), we need the
elimination and computations rules. Below is the 
elimination rule.   It defines a function called the \term{recursor}.
It is a kind of universal function-builder: given certain data as 
inputs, it produces a function \(f : \boolean \to C\) as output, namely 

%%% Line 373
\begin{equation}
f\, b = \rec_\boolean(t, f, b)
\end{equation}

%%% Line 376
\begin{equation}
\dfrac{C \type \quad t : C \quad f : C \quad b : \boolean}{\rec_\boolean(t, f, b): C}
\end{equation}

%%% Line 380
To complete the picture, we must give the computation rules:

%%% Line 382
\begin{equation}
\dfrac{t : C\qquad f : C}{{\rec_\boolean(t, f, \true) = t : C}}
\end{equation}

%%% Line 386
\begin{equation}
\dfrac{t : C\qquad f : C}{{\rec_\boolean(t, f, \false) = f : C}}
\end{equation}

%%% Line 390
The first computation rule says that if the last argument of \(\rec\)
is \(\true\), the return value is \(t: C\).   If it is false, the return value is \(f : C\).   Let us use what we have learned to define the function \(\nott : \boolean \to \boolean\) which negates its
argument:

%%% Line 394
\begin{equation}
\nott\, b = \rec_{\boolean}(\false,\true, b)
\end{equation}

%%% Line 397
Exercise: show that \(\nott \true ≡ \false\) and \(\nott \false ≡ \true\).

%%% Line 399
\textbf{Pattern matching.} We can also define \(\nott\) by pattern-matching:

%%% Line 401
\begin{align}
&\nott : \boolean \to \boolean\\
&\nott \true =\false\\
&\nott \false = \true
\end{align}

%%% Line 406
It is a mechanical procedure to translate pattern-matching
definitions into definition by eliminator.

%%% Line 409
\textbf{Boolean algebra}

%%% Line 411
Let's see if we can implement a fragment of the standard operations
in Boolean algebra. Below is how we might implement logical conjunction.   Here is one way to do it, using pattern-matching:

%%% Line 414
\begin{align}
&\and : \boolean \to \boolean \to \boolean\\
&\and \true \true = \true\\
&\and \true \false = \false\\
&\and \false \true= \false\\
&\and \false \false = \true
\end{align}

%%% Line 421
This style of pattern-matching can be improved:

%%% Line 423
\begin{align}
\label{\and-def}\\
&\and \, - \false = \false\\
&\and x \true = x
\end{align}

%%% Line 428


%%% Line 432
Here the first argument (underscore) of the first equation means 
\quote{ignore this value.}

%%% Line 435
As before, the function in question can also be defined
via the recursor:

%%% Line 438
\begin{equation}
\label{\rec-def-of-adn}\and = \lambda b_1 . \lambda b_2 . \rec_\boolean(b_2, \false, b_1)
\end{equation}

%%% Line 442
\begin{exercise}
Verify \eqref{rec-def-of-adn} the equations \eqref{and-def}.
\end{exercise}

%%% Line 445
A careful comparison of \eqref{rec-def-of-adn} and \eqref{and-def}
will show that the two derivations are interderviable.

%%% Line 448
\begin{exercise}
Define the logical function \(\or\) by (1) equations, (2) the eliminator.
\end{exercise}

%%% Line 451
Assuming the previous exercise, consider the following
sequence of reductions

%%% Line 454
\begin{align}
\label{bool-reductions}\\
&\or\ (\and\ (\or\ \true\ \false)\ \true)\ \false \to\\
&\or\ (\and\ \true\ \true)\ \false \to\\
&\or\ (\true\ \false) \to\\
&\true
\end{align}

%%% Line 461
Each line \eqref{bool-reductions} is a term of \(\boolean\).   However,
the last term is special: it cannot be further reduced.   Such a term is said to be in \term{normal form}.   Moreover, one can assert judgments like the below.   See the item (5) in section \ref{mltt-as-a-formal-system}.

%%% Line 464
\begin{equation}
\label{\boolean-computation}\or\ (\and\ (\or\ \true\ \false)\ \true)\ \false \equiv \true : \boolean
\end{equation}

%%% Line 468
The equality here, as noted in section \ref{mltt-as-a-formal-system},
is \u{definitional}. Note that in this case, computation of the boolean value on the left-hand-side of \eqref{boolean-computation}
is carreid out by a sequence of reductions to normal form.   

%%% Line 477
\section{Comparing Sets and Types} \label{comparing-sets-and-types}

%%% Line 479
Now that we have developed the rudiments of type theory,
let us compare it with set theory.   First,
in set theory, elements exist independently of any sets
into which they me gathered. Indeed, they exist
\emph{prior} to the creation of any set into which they are gathered.   Consider,for example, the natural 
numbers \(\set{}\), \(\set{\set\,}\).   They exist prior to their being gathered into the set of natural numbers \(\nat\).   The situation in type
theory is quite different.   One defines a type, then goes about
constructing terms, adding judgments to \(\mathcal{J}\) as time
proceeds. The type first, then the terms.

%%% Line 489
In set theory a thing like the number \(3\) can be a member of 
more than one set, e.g., the set of 
natural numbrers and the set of prime numbers.   In MLTT, a term
is forever tied to the term from hhisch it came.   It is the result
of application of constructors of that type and no other type,

%%% Line 501
\subsection{Russell's Paradox} \label{russells-paradox}

%%% Line 503
In 1902, Bertrand Russell wrote a letter to Gotlob Frege pointing out an error in Frege's work on the foundations of logic an mathematics.   Russell's letter introduced the set \(R = \sett{ x }{x \not\in x}\)
considered in section \ref{sets}.   The resulting problem --- \term{Russell's paradox} --- meant that there was a flaw in Frege's work on the foundations
of logic and mathematics
   The flaw arises from unrestricted self-reference.   To fix it, Russell devised a theory with
a hierarchy of types in which sets live.   It turns out that an early form of Martin-Löf type   theory
sufferred from a similar paradox due to Girard.   To resolve it, 
Martin-Löf introdcued the hierarchy of universes described in the
next section. I

%%% Line 512
The core idea of Russell's theory is that expressions in logic (and mathematics) are organized into a hierarchy of types:

\begin{itemize}

%%% Line 514
\item  Type 0: Individuals (basic objects)

%%% Line 516
\item Type 1: Sets (or predicates) of individuals

%%% Line 518
\item Type 2: Sets of sets of individuals
%%% Line 520
          And so on…

\end{itemize}

%%% Line 522
An object of a given type can only refer to or contain objects of lower types, never itself or those of the same or higher type.

%%% Line 525
\subsection{Universes} \label{universes}

%%% Line 528
A universe \(\cU\) is a type: one has 

%%% Line 530
\begin{equation}
\cU \type
\end{equation}

%%% Line 534
The terms of \(\cU\) are themselves types. If
\(A : \cU\), then \(A \type\). This setup 
lets you quantify over types safely.
To avoid self-reference and paradoxes, MLTT uses a cumulative hierarchy of universes:

%%% Line 539
\begin{equation}
\cU_0 : \cU_1 : \cU_2 : \ldots
\end{equation}

%%% Line 543
Each universe \(\cU_n\) is a term of the next universe \(\cU_{n+1}\).
 Types in \(\cU_n\) are also in \(\cU_{n+1}\), hence the hierarchy is 
cumulative. This stratification is analogous to Russell's type hierarchy: you can't have a universe that contains itself.

%%% Line 547
We say that \(\cU_n\) is a universe of \term{level} \(n\) and that
\(\cU_0\) is the \term{universe of small types}. The type of natural numbers
is small, as is the type of functions \(\nat \to \nat\). As we shall see below, there are \quote{large} types.

%%% Line 551
Having introduced universes, we must now modify the rules
of inference which goven types to take into account this new part of the theory.   Here is what we do for the formation and introduction 
rules for the natural numbers and for the construction of function types:

%%% Line 555
\begin{equation}
\label{U-formation}\dfrac{}{\nat : \cU_0}\qquad\dfrac{A : \cU_n \qquad B : \cU_n}{A \to B : \cU_n}
\end{equation}

%%% Line 561
Note that in the case of the function type, the type constructor \(\to\) preserves universe levels: If \(A\) and \(B\) are terms
of \(\cU_n\), then so is \(A \to B\). 

%%% Line 565
\subsection{Large Types} \label{large-types}

%%% Line 567
Consider a function \(f : A \to \cU_0\) where \(A : \cU_0\).   A first 
draft of its formaton rule is

%%% Line 570
\begin{equation}
\label{draft-large-\type}\dfrac{A: \cU_0 \qquad \cU_0:\cU_1}{A \to \cU_0 : \,??}
\end{equation}

%%% Line 574
We write \(B : \, ??\)   because the draft rule doe not fit the form
specified above.   But cumulativity saves the day: \(\cU_0 : \cU_1\)
and if \(A : \cU_0\), so \(A :\cU_1\).   Then \eqref{draft-large-type}
can be written as

%%% Line 579
\begin{equation}
\dfrac{A: \cU_1 \qquad \cU_0:\cU_1}{A \to \cU_0 : \,\cU_1}
\end{equation}

%%% Line 582
We conclude that \(f\) is a term of a type that lives in \(\cU_1\),
namely \(A \to \cU_0\).   Note that in this case the construction
still preserves type levels.

%%% Line 586
\begin{box}
Jumping ahead a bit, the \textbf{\term{ Identiy type}} is formed via 
the rule
%%% Line 590
\begin{equation}
\dfrac{a : A \quad b : A}{ a =_A b}
\end{equation}
%%% Line 593
   The introduction rule is 
%%% Line 595
\begin{equation}
\dfrac{a : A}{\refl_a : a =_A a}
\end{equation}
%%% Line 598
   We say that \(a\) and \(b\) are \textbf{\term{ propositionally equal}}
if \(a =_A b\) is \textbf{\term{ inhabited}}, meaning that there
is a term \(t : a =_A b\).   If \(a\) and \(b\) are definitionally
equal, i.e., if they have the same normal form, then 
\(a \equiv b\).   We conclude that \(a\) and \(b\) are equal types:
\(a =_A b \equiv a =_A a\).   The latter type is inhabited by
\(\refl_a\).   Therefore the former type is inhabited.   Therefore
\(a\) and \(b\) are propositionally equal.
Now consider the function \(f : \nat \to \cU_0\) given 
by \(f\, n :\equiv (0 =_\nat n)\).   Then \(f\) is a term 
in the large type \(\nat \to \cU_0\).
\par\par\par\par
\end{box}

%%% Line 614
\section{Primitive Recursion} \label{primitive-recursion}

%%% Line 617
We will now show that the natural pattern matching equations
for a function like the factorial are defined by a general
scheme called \term{primitive recursion}.   Such functions
are guaranteed to terminate on all well-typed inputs.
Once we have the primitive recursion scheme in hand, we will
tease out of it the elimination rule for the natural numbers.
Consider now the 
factorial function:

%%% Line 627
\begin{align}
&\fact : \nat \to \nat\\
&\fact \zero = \suc \zero\\
&\fact\; (\suc n) = (\suc n) * (\fact n)
\end{align}

%%% Line 632
We notice that there two defining equations, one for each
constructor of \(\nat\).   The second equation defines   \(\fact\)
in terms of itself.   However, the definition is not
circular becuase we notice, going in more detail, that
\(\fact\; (\suc n)\) is defined in terms of \(\fact n\). The
argument of \(\fact\) on the right is less than the argument
of \(\fact\) on the right.   Because there is a quantity which
decreases on each function call, the recursion is guaranteed to terminate after finitely many steps.   This is a feature of
of all functions that are definable in MLTT: computations 
always terminate.

%%% Line 643
Here is a sample computation:

%%% Line 645
\begin{align}
&\fact 3\\
&\fact\; (\suc\ 2)\\
&(\suc 2) * \fact 2\\
&3 * \fact\; (\suc 1)\\
&3 * (\suc 1) * \fact 1\\
&3 * 2 * \fact\; (\suc 0)\\
&3 * 2 * (\suc 0)  * (\fact 0)\\
&3 * 2 * 1 * 1
\end{align}

%%% Line 655
Let us now discover the general scheme via which this sort
of recursion works. We look once again at the definition:

%%% Line 658
\begin{align}
&\fact : \nat \to \nat\\
&\fact \zero = \suc \zero\\
&\fact\; (\suc n) = (\suc n) * (\fact n)
\end{align}

%%% Line 663
The first equation has the general form

%%% Line 665
\begin{equation}
f \zero = \base
\end{equation}

%%% Line 668
The second has the form

%%% Line 670
\begin{equation}
f (\suc n) = \step\; n\; (f\; n)
\end{equation}

%%% Line 673
where in this case 

%%% Line 675
\begin{equation}
\step n\; v = (\suc n) * v
\end{equation}

%%% Line 678
Notice that the type of \(\step\) is \(\nat \to \nat \to \nat\).
The recursion scheme for \(\fact\) is the general recursion
scheme displayed below, where \(C = \nat\):

%%% Line 682
\begin{box}

%%% Line 684
   Functions \(f : \nat \to C\), are defined by the equations
%%% Line 686
\begin{align}
&f \zero = \base\\
&f (\suc n) = \step n\; (f\; n)
\end{align}
%%% Line 690
   where
%%% Line 692
\begin{align}
& \base : C\\
& \step : \nat \to C \to C
\end{align}
\end{box}

%%% Line 696
This scheme is what is called \term{primitive recursion}. Functions
defined by this scheme terminate on any well-typed argument.

%%% Line 704
\subsection{The Non-Dependent Eliminator} \label{the-non-dependent-eliminator}

%%% Line 706
From our primitive recursion scheme as a guide, we can formulate
the elimination rule for the natural numbers.   We do this here for
non-dependent types.   Consider the problem of defining a function \(f : \nat \to C \to C\).   The data required to do this are

%%% Line 710
\begin{indent}
(i) a type \(C : \cU\)
\par\par
(ii) a term \(\base : C\)
\par\par
(iii) a function \(\step : \nat \to C \to C\)
\end{indent}

%%% Line 717
Let us make up a type for a function \(\rec_\nat\) --- the recursor ---
which takes the above data as input and produces a term of 
\(\nat \to C\) as output.   This straightforward:

%%% Line 721
\begin{equation}
\rec_\nat : (C: \cU) \to C \to (\nat \to C \to C) \to (\nat \to C)
\end{equation}

%%% Line 724
Then 

%%% Line 726
\begin{equation}
f\; n = \rec_\nat\; C\; \base\;  \step\;  n
\end{equation}

%%% Line 729
The computation rules are

%%% Line 731
\begin{align}
&\rec_\nat\; C\; \base\;  \step\;  0 = \base\\
&\rec_\nat\; C\; \base\;  \step\;  (\suc\; n) = \step\; n\; (\rec_\nat\; C\; \base\;  \step\;  n)
\end{align}

%%% Line 735
The factorial function is then defined as

%%% Line 737
\begin{equation}
\fact = \rec_\nat\; \nat\; \zero\; (\lambda\; n\; v \to (\suc n) * v)
\end{equation}

%%% Line 740
\subsection{Addition via the Eliminator} \label{addition-via-the-eliminator}

%%% Line 742
Let us see whether our scheme works for addition.   We recall the definition:

%%% Line 745
\begin{align}
&\add : \nat \to \nat \to \nat\\
&\add \zero n = n\\
&\add\; (\suc m)\; n = \suc\; (\add m \;n)
\end{align}

%%% Line 750
The type of \(\add\) is \(\nat \to \nat \to \nat\), which we
parenthesize as \(\nat \to (\nat \to \nat)\) and then
write as \(\nat \to C\) where \(C = \nat \to \nat\).   Notice that the value 
of \(\add m\) is of type \(C\), and that \(\add 0\) is the identity function.
Of the three pieces of data we need to for the recursor we have two:

%%% Line 756
\begin{equation}
\add = \rec_\nat\; C\; (\lambda n.n)\;\; ??
\end{equation}

%%% Line 759
where \(??\) is the function \(\step\).   Let us now find that function, keeping in mind that \(\step\) maps \(\add\; m\) as a function of \(n\) to 
\(\add\; (\suc\; m)\) as a function of \(n\):

%%% Line 762
\begin{equation}
\step n\; (f\; n) = f\; (\suc\; n)
\end{equation}

%%% Line 768
To do this, we write
the recursion equation defining \(\add\), then abstract with respect to 
the variable \(n\) to get an equality of functions:

%%% Line 772
\begin{align}
&\add\; (\suc m)\; n = \suc\; (\add\; m\; n)\\
&\lambda n.(\add\; (\suc m)\;
\end{align}

%%% Line 776
n) = \lambda n.(\suc\; (\add\; m\; n))

%%% Line 778
Now

%%% Line 780
\begin{align}
\step - (\lambda n.(\add\; m\; n)) &= \lambda n.(\add\; (\suc m)\; n)\\
&= \lambda n.(\suc\; (\add\; m\; n))\\
\step - (g\; n) &=  \lambda n.(\suc\; ( g \; n))
\end{align}

%%% Line 785
\subsection{Pattern-matching vs the Eliminator} \label{pattern-matching-vs-the-eliminator}

%%% Line 787
\section{Propositions as Types} \label{propositions-as-types}

%%% Line 789
Type theory defines its own logic via the doctrine of Propositions as Types:

\begin{itemize}

%%% Line 791
\item  \(\text{Propositions } P \longleftrightarrow \text{Types } P\)

%%% Line 793
\item \(\text{Proofs } p \text{ of } P \longleftrightarrow \text{Terms } p : P\)

%%% Line 795
\item \(\text{Falsehood} \longleftrightarrow \text{ Empty \type}\; \bot\)

\end{itemize}

%%% Line 797
The operations on propositions correspond to type forming operations, e.g.,

\begin{itemize}

%%% Line 799
\item Implication \(P \implies Q \longleftrightarrow A \to B\).

%%% Line 802
\item Negation \(\neg P \longleftrightarrow (P \to \bot)\)

\end{itemize}

%%% Line 805
\subsection{Propositional Logic} \label{propositional-logic}

%%% Line 807
Modus Ponens, one of the principles of classical logic, states that if
we know \(P\) and also \(P \implies Q\), then we know \(Q\).   This has an easy proof in MLTT. If we know \(P\), there is a witness \(p : P\).   If we know
\(P \implies Q\), there is a witness \(f : P \to Q\).   By the elimination rule for functions, \(f\; p : Q\).   Therefore we know \(Q\). \qed{}

%%% Line 811
The principle of the contrapositive is another part of classical logic which
has an easy formulation and proof in MLTT.   The principle states that if \(P \implies Q\), then \(\neg Q \implies \neg P\). For the proof, supoose that \(f : P \to Q\) and \(nq : Q \to \bot\). Then \(nq \circ f : P \to \bot\). \qed{}

%%% Line 815
\subsection{Currry-Howard Correspondence} \label{currry-howard-correspondence}

%%% Line 817
The sketch of propositional logic just given
illustrates the promise that the doctrine of 
Propositions as Types holds.   The Curry-Howard
correspondence, set forth in the table
below, shows how this doctrine suffics
to translate all the notions of the predicate
calculus into type theory.

%%% Line 828
error in constructing table

%%% Line 839
The logic thus defined is a \term{constructive logic}. Such logics
are built on the notion of proof rather
than truth, as in classical logic.   As a result, the Law of the Excluded Middle (LEM) does not hold, and negation must be
handled with greater care.

%%% Line 844
In this section we discuss salient differences between classical logic and
the constructive logic of MLTT.   In the succeeding sections we fill out the parts
of the Curry-Howard Correspondence that we have not yet treated.

%%% Line 850
\subsection{Negation} \label{negation}

%%% Line 852
We have already seen that the implication 
\((P \to Q) \to (\neg Q \to \neg P)\) holds in MLTT.
In this section we look more closely at negation in MLTT
as compare to classical logic.   The first point is that 
while \(A \to \neg\neg A\) holds in MLTT for all \(A\), the 
principle \(\neg\neg A \to A\) does not.   Thus \(\neg\neg A\) 
and \(A\) are not logically equivalent, as they are in classical logic.

%%% Line 860
\begin{lemma}
\label{lemma-double-neg}
In classical logic, LEM implies both the weak and strong
forms of the Law of Double Negation, \(A \to \neg\neg A\) 
and \(\neg\neg A \to A\), respectively.
\end{lemma}

%%% Line 867
\textbf{Proof.}
\par\par
1. (Strong form). Assume \(\neg \neg A\) is true.   Then \(\neg A\) is false.
By LEM, either \(A\) or \(\neg A\) is true. Since \(\neg A\)
is false, LEM demands that \(A\) be true.   We have proved that \(\neg\neg A \to A\)/
\par\par
2. (Weak form). Assume \(A\) is true.   Then \(\neg A\) is false.   In LEM, 
replace \(A\) by \(\neg A\) to conclude that \(\neg A \lor \neg \neg A\) is true.
Since \(\neg A\) is false, \(\neg \neg A\) must be true.
We have proved that \(A \to \neg\neg A\).
\par\par
\qed{}

%%% Line 881
The type-theoretical translation of Lemma \ref{lemma-double-neg}
states that the types representing 
both strong and weak double negation are inhabited. The argument
below shows that weak double negation holds in MLTT. The strong
form does not, as we argue later in this section 
\ref{law-of-the-excluded-middle} and \ref{heyting-theorem}

%%% Line 888
\begin{proposition}
In MLTT, the Weak Law of Double Negation holds.   That is, 
\(A \to \neg \neg A\) is inhabited.
\end{proposition}

%%% Line 893
\textbf{Proof.} The goal is to construct a term of 

%%% Line 895
\begin{equation}
A \to (A \to \emptyt) \to \emptyt
\end{equation}

%%% Line 898
\textit{First Proof:} Given \(a : A\) and \(f : A \to \emptyt\), we have \(f(a) : \emptyt\). Thus we
have a term of type \(A \to (A \to \emptyt) \to \emptyt\), as required. (2) Alternatively, consider
the \(\lambda\)-term \(t = \lambda (a:A).\lambda (f:A \to \emptyt).f\,a\). Then \(t : A \to (A \to \emptyt) \to \emptyt\),
as required.
\par\par
Q.E.D.
Below is the proof in Agda

%%% Line 908
\begin{verbatim}
dnn : \{A : Set\} → A → ¬ (¬ A)
dnn  a f = f a
\end{verbatim}

%%% Line 912
The code makes more sense if we expand the declaration:

%%% Line 914
\begin{verbatim}
dnn : \{A : Set\} → A → (A → ⊥) → ⊥
dnn  a f = f a
\end{verbatim}

%%% Line 919
\section{Identity Type} \label{identity-type}

%%% Line 921
Consider for moment these two assertions, both 
definitional equalities.

%%% Line 924
\begin{indent}
(i) \(\zero + n \equiv n\)
\par\par
(ii) \(n + \zero \equiv n\)
\end{indent}

%%% Line 930
The first is true: the normal form the left-hand side is \(\zero\)
as is the normal form of the right-hand side.   The second is false, because
if \(n\) is not \(\zero\), there are no rules to reduce the left-hand side.
It is because of (ii) that we need a more powerful form of equalit.   This
is given to us by the \term{identity type}:

%%% Line 936
\begin{equation}
\frac{A: \cU \quad a : A \quad b :A}{a =_A b: \cU}
\end{equation}

%%% Line 939
The identity type is a \term{dependent} type: its definition depends
on terms of other types.   Notice that formation of the identity types
is an operation within the given universe.      Constructors are given only for the type \(a = a\), as in the equation below. 

%%% Line 943
\begin{equation}
\frac{A : \cU \qquad a : A}{\refl_a : a = a}
\end{equation}

%%% Line 946
If the identity type \(a = b\) is \term{inhabited}, that is, if thre is a term \(p : a = b\) then we say that \(a\) and \(b\) are \term{propositionally equal}.   We claim, and will soon prove, that proposiional equality is 
an equivalance relation.   It is, moreover, a \term{congruence}: if \(f : A \to B\), there is a function \(\congg f : x =_A y \to f\; x =_B f\;   y\).   Thus, if \(p : x =_A y\), then \(\congg f\;      p : f\; x =_B f\;   y\),

%%% Line 949
Suppose that \(a\) and \(b\) are defiinitionally equal: \(a \equiv b\).
The the types \(a = b\) and \(a = a\) are definitionally equal. The latter type is inhabited by \(\refl_a\). The former type, which is equal to \(a = a\),
is therefore also inhabited.   We have just proved that 

%%% Line 953
\begin{indent}
\textbf{Principle:} \textit{If\(a\) and \(b\) are definitionally equal, then they are also propositionally equal.}
\end{indent}

%%% Line 956
Consider now the type family \(\isRightIdentity n = n + \zero =_\nat n\). 
We aim to show that there is a function \(\lemma : (n : \nat) \to \isRightIdentity n\).   In other words, we claim that the type 
\((n : \nat) \to \isRightIdentity n\) is inhabited.   If that is so, we have proved that 

%%% Line 960
\begin{equation}
\forall n : \nat, n + 0 \text{ is propositionaly equal \to } n
\end{equation}

%%% Line 963
The proof is by induction.   If \(n = 0\), then \(\lemma n = 0 + 0 =_\nat 0\)
is inhabited by \(\refl_0\).   Assume as inductive hypothesis that
\(\lemma n\) is inhabited by \(p\). Then we have

%%% Line 967
\begin{align}
\congg \suc (n + 0 =_\nat 0) &= \suc\; (n + 0) =_\nat \suc n\\
&= (\suc n) + 0 =_\nat = \suc n\\
&= \lemma\; (\suc n)
\end{align}

%%% Line 972
In other words, we have shown that

%%% Line 974
\begin{equation}
\congg\; \suc\; (\lemma n) = \lemma\; (\suc n)
\end{equation}

%%% Line 977
Consequently \(\lemma n\) is proved for all \(n\).   

%%% Line 979
There is one more
bit of information that we can extract from this argument:

%%% Line 982
\begin{indent}
\textit{The term which inhabits\(\lemma n\) is \((\congg \suc)^n \refl_0\)}
\end{indent}

%%% Line 985
As a result, we know not only that \(n + 0\) 
is propositionally equal to \(n\), but
we have in hand a proof term for this fact.   Notice that the proof term
is not \(\refl_x\) for some \(x\), but rather
some function applied to \(\refl_0\).   That is why
both of the following hold: (1) \quote{the identity type is freely generated by\(\refl\)} and (2) \(n + 0\) is propositionally equal to \(n\)
but not definitionally equal. Think of the identity type as being
like a cyclic module \(M\) over a ring \(R\) --- that is a module with one generator \(m\).   An arbitrary
module element has the form \(rm\).   There are as many elements
of this cyclic module as there are elements of the ring \(R\).

%%% Line 998
\subsection{Induction Principle} \label{induction-principle}

%%% Line 1001
Consider once again the congruence principle.   It is the function

%%% Line 1003
\begin{equation}
\congg : (f : A \to B) \to (x\; y : A) \to (p : x =_A y) \to f\; x =_B f\; y
\end{equation}

%%% Line 1006
If we apply \(\congg\) to \(f\), we have

%%% Line 1008
\begin{equation}
\congg f : (x\; y : A) \to (p : x =_A y) \to f\; x =_B f\; y
\end{equation}

%%% Line 1012
This function has the form

%%% Line 1014
\begin{equation}
\congg f : (x\; y : A) \to (p : x =_A y) \to C\; x\; y\; p
\end{equation}

%%% Line 1017
where the last type is

%%% Line 1019
\begin{equation}
C\; x\; y\; p :\equiv f\; x =_B f\; y
\end{equation}

%%% Line 1022
In this case there is no dependence on \(p\). Thus constructing the function \(\congg\) is a special case of constructing functions

%%% Line 1024
\begin{equation}
f : (x, y: A) \to (p : x =_A y) \to C(x, y, p)
\end{equation}

%%% Line 1027
where \(C(x,y,p)\) is a family of types depending on \(x, y : A\)
and \(p: x =_A y\). Proofs for symmetry and transitivity requires
exactly the same construction. To construct the output \(f\), we need certain inputs:

\begin{enumerate}

%%% Line 1031
\item The type family \(C : (x, y : A) \to (x =_A y) \to \cU\)

%%% Line 1033
\item A function \(c : (x : A) \to C(x,x,\refl_x)\)

\end{enumerate}

%%% Line 1035
The type family is the one appearing in the type of \(f\), so the 
new element is the function \(c\).   It assigns to 
\(\refl_x : x =_A x\) a term of type \(C(x,x,\refl_x)\).
That is all the data needed to construct \(f\).
The induction principle for the identity type states that given
these inputs, there is a function \(f\) of the type indicated above which satisfies

%%% Line 1042
\begin{equation}
f(x,x,\refl_x) :\equiv c(x)
\end{equation}

%%% Line 1045
\subsection{Congruence} \label{congruence}

%%% Line 1047
Consider the hypothetical function \(\congg f\):

%%% Line 1049
\begin{equation}
\congg f : (x\; y : A) \to (p : x =_A y) \to f\; x =_B f\; y
\end{equation}

%%% Line 1053
Let us construct it using the induction principle.   For \(C\), 
we choose the family \(C\; x\; y = f\; x =_B f\; y\) and we set

%%% Line 1056
\begin{equation}
c\; x = \refl_{(f\, x)} : C\; x\; x
\end{equation}

%%% Line 1059
This makes sense because \(C\; x\; x = (f\; x =_B f\; x)\).

%%% Line 1061
\textbf{Alternate method of proof.} There is an equivalent, alternate method of proof which goes as follows.      To construct

%%% Line 1063
\begin{equation}
\congg f : (x\; y : A) \to (p : x =_A y) \to f\; x =_B f\; y
\end{equation}

%%% Line 1066
it suffices to consider the case where \(x\) and \(y\) are the same
and \(p = \refl_x\).   Then we have

%%% Line 1069
\begin{equation}
\congg f : (x\; x : A) \to (p : x =_A x) \to f\; x =_B f\; x
\end{equation}

%%% Line 1072
It suffices to set 

%%% Line 1074
\begin{equation}
\congg f : x\; \refl_x = \refl_{(f x)}
\end{equation}

%%% Line 1078
\subsection{Symmetry} \label{symmetry}

%%% Line 1080
Let us see how the more informal method works to prove symmetry.   What we
must prove is that the following function exists: 

%%% Line 1083
\begin{equation}
\sym : (x : A) \to (y : A) \to (x =_A y) \to (y =_A x)
\end{equation}

%%% Line 1086
When \(x\) and \(y\) are the same, this reads

%%% Line 1088
\begin{equation}
\sym : (x : A) \to (x : A) \to (x =_A x) \to (x =_A x)
\end{equation}

%%% Line 1091
We set

%%% Line 1093
\begin{equation}
\sym\;x\; x\; \refl_x = \refl_x
\end{equation}

%%% Line 1096
Then \(\sym\) is defined when \(x = y\) and therefore defined for all \(x\)
and \(y\).

%%% Line 1099
\subsection{Transitivity} \label{transitivity}

%%% Line 1101
\begin{equation}
\trans : (x : A) \to (y : A)\to (z : A)\to (x =_A y) \to (y =_A z) \to (x =_A z)
\end{equation}

%%% Line 1104
Take \(y = x\), \(p = \refl_x\) and substitute, letting
\(f\; z\; q = \trans\, x\; x\; z\; \refl_x q\) where
\(f : (z : A) \to (x =_A z) \to (x =_A z)\). Now take \(z = x\).   Then the type of the second argument of \(f\) is \(x =_A x\), which is inhabited 
by \(\refl_x\).   Set 

%%% Line 1109
\begin{equation}
f\; x \refl_x = \refl_x
\end{equation}

%%% Line 1112
To conclude,

%%% Line 1114
\begin{equation}
\trans\; x\; x\; x\; \refl_x \;\refl_x = \refl_x
\end{equation}

%%% Line 1118
\section{Induction Principle for \(\nat\)} \label{induction-principle-for-nat}

%%% Line 1120
Recall that to define a function \(f : \nat \to C\), where \(C\)
is some type, we use the elimination rule in the form of the recursor:

%%% Line 1124
\begin{equation}
\rec_\nat : (C: \cU) \to C \to (\nat \to C \to C) \to (\nat \to C)
\end{equation}

%%% Line 1127
To construct proofs of assertions about the natural numbers,
we need the elimination rule in its full power, namely where \(C: \nat \to \cU\) is ia type family. The   output of the rule is a dependent function 

%%% Line 1130
\begin{equation}
f : (n : \nat) \to C\; n
\end{equation}

%%% Line 1133
The inputs are

\begin{enumerate}

%%% Line 1135
\item a type family \(C : \nat \to \cU\)

%%% Line 1137
\item a term \(\base: C\; \zero\)

%%% Line 1139
\item a function \(\step : (n : \nat) \to C\; n \to C (\suc\; n\))

\end{enumerate}

%%% Line 1141
The elimination rule for the natural numbers and type families
can be read off this input-output data:

%%% Line 1144
\begin{equation}
\ind_\nat (C : \nat \to \cU) \to C\; \zero \to ((n: \nat) \to C\; n \to C (\suc n)) \to ((n : \nat) \to C)
\end{equation}

%%% Line 1147
The computation rules are

%%% Line 1149
\begin{align}
&\ind_\nat\; C\; \base\, \step\, \zero =  \base\\
&\ind_\nat\; C\; \base\, \step\, (\suc n) =  \step n\; (\ind_\nat\; C\; \base
\end{align}

%%% Line 1153
Unsurprisingly, these are of the same form as the computaton rules for 
the recursor.

%%% Line 1157
\section{Theory and Metatheory} \label{theory-and-metatheory}

%%% Line 1159
\subsection{Normal Forms} \label{normal-forms}

%%% Line 1161
We defined definitional equality in terms of normal forms.   That begs
some foundational questions: (1) do the exist? (2) are they unique?
To see that there is an issue, let us consider the lambda calculus once again.   Let \(\Omega = \lambda x. x\, x\)   Then we have the reduction sequence

%%% Line 1165
\begin{equation}
\Omega \Omega \xrightarrow{\beta}\Omega \Omega \xrightarrow{\beta} \Omega \Omega \xrightarrow{\beta}\ldots
\end{equation}

%%% Line 1170
The reduction sequence never terminates, and so \(\Omega\Omega\) has 
no normal form. 

%%% Line 1173
In MLTT, normal forms consist entirely of constructors.

%%% Line 1175
\subsection{Typability SLTC} \label{typability-sltc}

%%% Line 1177
This and other considerations led Curch to 
formulate the simply-type lambda calculus (SLTC).   The syntax 
rules change slightly: abstractions carry a type annotation.
Instead of \(\lambda x.x\), we write \(\lambda x : T\), where \(T\)
is a type.   In addition, there are two \term{typing rules}:

%%% Line 1183
\begin{equation}
\frac{x : A \vdash e : B}{(\lambda x: A.e): (A \to B)}\qquad\frac{f : A \to B \quad e : A}{f\; e :B}
\end{equation}

%%% Line 1188
The first of these   assings a type to   functions.   The second defines the type of a function application.   Let's take a second look at
\(\Omega = \lambda x.x\,x\).   What is its type? Let us assume that \(x\)
has type \(A\).   Since \(x\,x\) is an application, \(x\) also has type \(X \to Y\) for some \(X\) and \(Y\). Since \(x\) is applied to \(x\), which has type \(A\), we conclude that \(X = A\). Therefore \(A = A \to Y\).   We try to solve this equation for \(A\), the type of \(x\): 

%%% Line 1192
\begin{equation}
A = A \to Y = (A \to Y) \to Y = ((A \to Y) \to Y) \to Y \cdots
\end{equation}

%%% Line 1195
There is no solution, so \(x\) is not typeable and therefore \(\Omega\) is
not typeable.

%%% Line 1198
\subsection{Typeability in Practice} \label{typeability-in-practice}

%%% Line 1200
Consider functions \(\double : \nat \to \nat\) and \(\isEven : \nat \to \boolean\).   Let's ask whether the expressions (1) \(\isEven\; (\double 3)\)
and (2) \(\double\; (\isEven 3)\) are typeable. The tree for typeing the first of these is

%%% Line 1203
\begin{equation}
\dfrac{\nat \to \boolean \qquad \dfrac{\nat \to \nat \quad \nat}{\nat}}{\boolean}
\end{equation}

%%% Line 1206
For the second it is 

%%% Line 1208
\begin{equation}
\dfrac{\nat \to \nat \qquad \dfrac{\nat \to \boolean \quad \nat}{\boolean}}{\text{Stuck!}}
\end{equation}

%%% Line 1211
In the second case, for \(\double\; (\isEven 3)\), we reach a stage in computing the tree of types (from the leaves of the tree to the root)
beyond which we cannot proceed.   We are stuck, and so the expression
in turn is not typeable.

%%% Line 1215
\subsection{Strong normalization} \label{strong-normalization}

%%% Line 1217
We say that a formal system satisfies \term{strong normalization} if every well-typed term reduces to normal form in finitely many steps. Both MLTT
and SLTC are strongly normalizing.      

%%% Line 1221
\subsection{Confluence} \label{confluence}

%%% Line 1223
Suppose that a term \(t\) has reductions \(t \to\!\!*\,u_1\) and 
\(t \to\!\!*\, u_2\). The symbol \(\to\!\!*\,\) means a sequence of reductions.   Supposer further that there is a term \(v\) and reductions
\(u_1 \to\!\!*\, v\) and \(u_2 \to\!\!*\, v\). If there is such a \(v\) for any 
pair of reductions of \(t\), we say that the system is \term{confluent} (or satisfies the Church-Rosser property).   MLTT and SLTC are both confluent.   This means that normal forms are unique 

%%% Line 1229
\subsection{Consistency} \label{consistency}

%%% Line 1231
A system is consistent if it is not possible to derive a contradiction.   By this we mean to derive both a proposition \(P\) and its negation \(\neg P\).
Suppose that this is possible in MLTT. Then we have witnesses \(p : P\)
and \(f : P \to \bot\).   Then \(f\; p\) is a term of \(\bot\).   Therefore, if
MLTT is inconsistent, the empty type is inhabited.   Conversely, if the empty type is inhabited, then all propositions are provable.   This follows from the elimination principle for the empty type.   If all propositons are provable, one may prove both \(P\) and \(\neg P\).

%%% Line 1236
Suppose that there is a witness \(p\) for the empty type.   By strong
normalization, \(p\) reduces to a normal form of \(\bot\).   Normal forms
consist entirely of constructors.   But \(\bot\) has no constructors.
Arguing by contradciton, we conclude that MLTT is consistent.

%%% Line 1241
\textbf{Note.} The argument just given applies to pure MLTT. If one adds
axioms like function extensionality or univalence, as in homotopy type theory, strong normalization is lost.

%%% Line 1245
\subsection{Structural Rules} \label{structural-rules}

%%% Line 1247
In addition the type-specific rules (formation, introduction, elimination,
and computation), MLTT has various structural rules.

\begin{itemize}

%%% Line 1250
\item \textbf{\term{ Disjontness.}}   If a type has multiple constructors, e.g. \(\zero\) and \(\suc\), their outputs cannot be equal, eg. \(\zero \ne \suc n\) for any \(n\)

%%% Line 1252
\item \textbf{Injectivity of construtors.} If \(c x = c y\), then \(x = y\).   This important for pattern-matching.

\end{itemize}

%%% Line 1254
\subsection{Metatheory} \label{metatheory}

%%% Line 1256
Metatheory is theory about theory.

\begin{itemize}

%%% Line 1258
\item Does every well-typed term   reduce to normal form?

%%% Line 1260
\item Is the type-checking algorithm decideable?   A decideable algorithm determines, in finitely many steps, whether a give term \(t\) has type \(A\) in aa given context \(\Gamma\).   That is, for given \(t\), \(\Gamma\), and \(A\), does \(\Gamma \vdash t : A\) hold?

%%% Line 1264
\item Is substitution admissble (do judgments respect variable replacement).

%%% Line 1266
\item Termination.   In MLTT, termination is enforced by structural recursion or by some well-founded measure that decreases on each function call.

%%% Line 1269
\item Totality.   Consdier the (Haskell) function \(f\; n = \text{if } n == 0 \text{ then } 1 \text{ else } f ( n - 1)\). It is \textbf{\term{ total}} over \(\nat\): it terminates and is defined for all \(n : \nat\). Now consider \(g\; n = \text{if } n == 0 \text{ then } 1 \text{ else } g\, n\). This function is not total: it loops forever on input \(n > 0\).   In MLTT,   functions are total by design. The type checker will exclude functions with missing cases or which loop forever.   Thus non-total functions are not definable in Elm.

\end{itemize}

%%% Line 1274
\section{Models and LEM} \label{models-and-lem}

%%% Line 1276
A model \(\cM\) of a formal system \(\cF\) is a structure-preserving 
map \(\nu : \cF \to \cA\) where \(\cA\) is some kind of algebraic object 
such as a Boolean or Heyting algebra.   

\begin{itemize}

%%% Line 1280
\item \textbf{Consistency.} \(\cF\) has at least one model.

%%% Line 1282
\item \textbf{Soundness.} \(\Gamma \vdash \varphi \implies \Gamma \models \varphi\), that is, if \(\varphi\) is derivable, it is true in all models.

%%% Line 1285
\item \textbf{Completeness.}   \(\Gamma \models \varphi \implies \Gamma \vdash \varphi\) that is, if \(\varphi\) is true in all models, it is derivable.

\end{itemize}

%%% Line 1287
Propositional logic is consistent, sound, and complete.

%%% Line 1290
error in constructing table

%%% Line 1297
Each row in the table has a difrent assignment of truht 
values to \(p\) and \(q\).   Each row is a model.   In this case there
are just \(2\) propositional variables \(p\) and \(q\), so there are only \(4\) models.   The column for \(\leftrightarrow\) consists entirely of \(T\)'s, so we can say that \(\models p \land q \leftrightarrow q \land p\) --- this formula is true in all models.

%%% Line 1301
\subsection{Heyting Models and LEM} \label{heyting-models-and-lem}

%%% Line 1303
Let \(\cA\) the Heyting algebra of open sets of a topological space \(X\).   The   operations are union, intersection, and Heyting complement: If \(U\) is an open set, then \(\neg U\) is the interior of the complement of the set-theoretic complement of \(U\).   A Heying model of a propositional calculus \(\cP\) is a map \(\cO : \cP \to \cA\) that sends propositions to open sets
and respects logical connectives on the left and union, intersection, and Heyting complement on the right.   One has \(\cO(T) = X\) and \(\cO(F) = \empty\).   Moreover, axioms map to \(\cO(T)\), and inference rules preserve truth: if the premises map to \(\cO(T)\), then so does the conclusion.

%%% Line 1306
Consider the law of the excluded middle (LEM). It says that for all propositions \(P\), \(P \lor \neg P\) holds.   In such a system, \(\cO(P) \cup \neg \cO(P) = X\).   

%%% Line 1308
Now consider a system like intuitionistic logic in whch LEM is not an
axiom.   We ask: is it derivable in intuitionistic logic..   This is a question that model theory
can answer.   Intuitionistic logic has axioms and inference rules,
and one can set models so that these are preserved. Choose such a model 
so that a distinguised proposition \(P\) maps to the open set \(x > 0\).
Then 

%%% Line 1315
\begin{equation}
\cO (\neg P) \cup (P) = \sett{x \in \bR}{ x < 0} \cup \sett{x \in \bR}{ x >  0} = \sett{x \in \bR}{ x \ne 0}
\end{equation}

%%% Line 1318
Consequently LEM is not derivalble in intuitonistic logic.

%%% Line 1323
\begin{remark}
If the topology of \(X\) is discrete, then the set-theoretic 
complement and the Heyting complement are the same. LEM cannot be
refuted by such models.
\end{remark}

%%% Line 1332
\section{References} \label{references}

%%% Line 1334
\textbf{Models}

%%% Line 1336
\href{https://www2.mathematik.tu-darmstadt.de/~streicher/}{Hoffman and Streicher}

%%% Line 1338
\textbf{Hedberg's Theorem}

%%% Line 1340
\href{https://martinescardo.github.io/GeneralizedHedberg/html/GeneralizedHedberg.html}{Escardo et al, Generalized Hedberg}

%%% Line 1342
\href{https://github.com/agda/agda-stdlib/blob/master/src/Axiom/UniquenessOfIdentityProofs.agda}{UIP@stdlib}

%%% Line 1344
\href{https://www.cs.uoregon.edu/research/summerschool/summer14/rwh_notes/notes_week9.pdf}{Harper's U Oregon Course}

%%% Line 1346
\href{https://www.andrew.cmu.edu/user/awodey/hott/papers/hedberg.pdf}{Hedberg's article}

%%% Line 1348
\href{https://doisinkidney.com/code/probability/Cubical.Relation.Nullary.DecidableEq.html}{essence of the proof using CuTT}

%%% Line 1350
\href{https://www.cs.uoregon.edu/research/summerschool/summer14/rwh_notes/notes_week5.pdf}{U Orego course partial summary}

%%% Line 1352
\href{https://www.cse.chalmers.se/~nad/listings/equality/Equality.Decidable-UIP.html}{chalmers cubical hedberg}

%%% Line 1354
\href{https://math.stackexchange.com/questions/2208376/what-am-i-not-understanding-about-hedbergs-theorem}{stackexchange}

%%% Line 1356
\href{https://planetmath.org/72uniquenessofidentityproofsandhedbergstheorem}{planetmath}

%%% Line 1358
\href{https://scs.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=ac7edbff-ef18-4204-b2d4-d45f58bf3b34}{Harper Lecture}

%%% Line 1360
\href{https://www.math.fsu.edu/~ealdrov/teaching/2020-21/fall/MAS5932/agda/sets-logic.html#hedberg}{FSU notes}

%%% Line 1362
\href{https://martinescardo.github.io/papers/hedberg.pdf}{Escardo etal generalizaitons of Hedberg's theorem}

%%% Line 1365
\href{https://git.mzhang.io/michael/type-theory/src/commit/f10e3a09b9c83d8534466057d40e4fc7e5cde5af/src/HoTTEST/Agda/Lecture-Notes/files/Hedbergs-Theorem.lagda.md}{Zhang cubical Hedberg}

\clearpage

\printindex



\end{document}
