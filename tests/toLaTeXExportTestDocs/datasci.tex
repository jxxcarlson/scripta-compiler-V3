\documentclass[11pt, oneside]{article}
\usepackage{float}
\usepackage{graphicx}
\usepackage{wrapfig}
\graphicspath{ {image/} }



%% Packages

\usepackage{listings}

%% Standard packages
\usepackage{geometry}
\geometry{letterpaper}
\usepackage{changepage}  % for the adjustwidth environment
\usepackage{graphicx}    % for \includegraphics

%% Index, hyperref
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{hyperref}   % load before imakeidx
\usepackage{imakeidx}

%%%%
\usepackage[normalem]{ulem} % for \st
\usepackage{soul}           % for \hl and \sethlcolor
\usepackage{wrapfig}        % for wrapfigure

%% AMS
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{amscd}

\usepackage{fancyvrb} %% for inline verbatim
\usepackage{makeidx}

%% Chemistry
\usepackage[version=4]{mhchem} % for \ce



%% Commands

\newcommand{\hang}[1]{%
  {%
    \setlength{\leftskip}{1em}%
    \setlength{\hangindent}{1em}%
    \hangafter=1 %
    #1\ \vpace{4}%
  }%
}

\renewcommand{\labelitemi}{\scalebox{0.7}{\textbullet}}

% Dot box = 1em, gap = 1em → total = 2em
\newcommand{\compactItem}[1]{%
  \par
\noindent
  \hangindent=2em \hangafter=1%
  \makebox[1em][l]{\labelitemi}\hspace{1em}#1\par
}

\newcommand{\code}[1]{{\tt #1}}
\newcommand{\ellie}[1]{\href{#1}{Link to Ellie}}
% \newcommand{\image}[3]{\includegraphics[width=3cm]{#1}}

%% width=4truein,keepaspectratio]


% imagecentercaptioned command removed - using standard figure environment instead

\newcommand{\imagecenter}[2]{
   \medskip
   \begin{figure}[htp]
   \centering
    \includegraphics[width=#2]{#1}
    \vglue0pt
    \end{figure}
    \medskip
}

\newcommand{\imagefloat}[4]{
    \begin{wrapfigure}{#4}{#2}
    \includegraphics[width=#2]{#1}
    \caption{#3}
    \end{wrapfigure}
}


\newcommand{\imagefloatright}[3]{
    \begin{wrapfigure}{R}{0.30\textwidth}
    \includegraphics[width=0.30\textwidth]{#1}
    \caption{#2}
    \end{wrapfigure}
}

\newcommand{\hide}[1]{}


\newcommand{\imagefloatleft}[3]{
    \begin{wrapfigure}{L}{0.3-\textwidth}
    \includegraphics[width=0.30\textwidth]{#1}
    \caption{#2}
    \end{wrapfigure}
}
% Font style
\newcommand{\italic}[1]{{\sl #1}}
\newcommand{\strong}[1]{{\bf #1}}
\newcommand{\strike}[1]{\st{#1}}

% Scripta
\newcommand{\ilink}[2]{\href{{https://scripta.io/s/#1}}{#2}}
\newcommand{\markwith}[1]{}
\newcommand{\anchor}[1]{#1}

% Color
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\violet}[1]{\textcolor{violet}{#1}}
\newcommand{\highlight}[1]{\hl{#1}}
\newcommand{\note}[2]{\textcolor{blue}{#1}{\hl{#1}}}

% WTF?
\newcommand{\remote}[1]{\textcolor{red}{#1}}
\newcommand{\local}[1]{\textcolor{blue}{#1}}

% Unclassified
\newcommand{\subheading}[1]{{\bf #1}\par}
%\newcommand{\term}[1]{{\index{#1}}}
%\newcommand{\termx}[1]{}
\newcommand{\comment}[1]{}
\newcommand{\innertableofcontents}{}


% Special character
\newcommand{\dollarSign}[0]{{\$}}
\newcommand{\backTick}[0]{\`{}}

%% Theorems
\newtheorem{remark}{Remark}
\newtheorem{theorem}{Theorem}
\newtheorem{axiom}{Axiom}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{problem}{Problem}
\newtheorem{exercises}{Exercises}
\newcommand{\bs}[1]{$\backslash$#1}
\newcommand{\texarg}[1]{\{#1\}}


%% Environments
\renewenvironment{quotation}
  {\begin{adjustwidth}{2cm}{} \footnotesize}
  {\end{adjustwidth}}

\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\let\endchangemargin=\endlist

\renewenvironment{indent}
  {\begin{adjustwidth}{0.75cm}{}}
  {\end{adjustwidth}}


%% NEWCOMMAND

% \definecolor{mypink1}{rgb}{0.858, 0.188, 0.478}
% \definecolor{mypink2}{RGB}{219, 48, 122}
\newcommand{\fontRGB}[4]{
    \definecolor{mycolor}{RGB}{#1, #2, #3}
    \textcolor{mycolor}{#4}
    }

\newcommand{\highlightRGB}[4]{
    \definecolor{mycolor}{RGB}{#1, #2, #3}
    \sethlcolor{mycolor}
    \hl{#4}
     \sethlcolor{yellow}
    }

\newcommand{\gray}[2]{
\definecolor{mygray}{gray}{#1}
\textcolor{mygray}{#2}
}

\newcommand{\white}[1]{\gray{1}[#1]}
\newcommand{\medgray}[1]{\gray{0.5}[#1]}
\newcommand{\black}[1]{\gray{0}[#1]}

% Spacing
\parindent0pt
\parskip5pt

\makeindex[
                          title=Index,
                          columns=2,
                          %% intoc     % include index in the table of contents
                        ]

\begin{document}

\title{Untitled}

\date{}

\author{
test-author
}

\maketitle

\tableofcontents

%%% Line 5
\newcommand{\reals}{\mathbb{R}}
\newcommand{\ba}{\mathbf a}
\newcommand{\bu}{\mathbf u}
\newcommand{\bv}{\mathbf v}
\newcommand{\bw}{\mathbf w}
\newcommand{\bfx}{\mathbf x}
\newcommand{\by}{\mathbf y}
\newcommand{\sett}[2]{\{\  {#1} \ |\ {#2} \ \}}



%%% Line 3


%%% Line 5


%%% Line 17
\textit{The word dimension stems from the Latin word dīmensiō. It is derived from dīmetīrī, meaning ``to measure out.''
In data science, a vector in\(n\)-space is typically an ordered list of measurements \((v_1, v_2, \ldots, v_n)\).}

%%% Line 20
\section{Prologue} \label{prologue}

%%% Line 22
The goal of this chapter is to introduce the basic notions
of calculus and linear algebra, developing just enough
vocabulary to show some applications to data science and prepare
the way for more advanced work.   Among these applications are:

\begin{itemize}

%%% Line 28
\item gradient descent

%%% Line 31
\item clustering algorithms

%%% Line 33
\item finding good projections by principal component analysis (PCA).

\end{itemize}

%%% Line 35
Gradient descent is (among other things) part of the technology that makes it possible for neural networks to learn.
Clustering algorithms are used to find patterns in data, resolving
"clouds" of data points into subclouds of points that are related
to eachother. PCA is used to reduce
the dimension of large data sets so that computation with them becomes
feasible and
also to visualize them by projecting
data from \(n\) dimensions to two or three.   PCA can also help to find
meaningful ways of describing the data.

%%% Line 45
\section{Calculus} \label{calculus}

%%% Line 47
Let's begin with calculus, the basic notions of which are the \textit{derivative} and the \textit{integral}.   In this introductory chapter,
we will only talk about derivatives and integrals for functions
of one variable.   But it will be important for applications
to data science and machine learning to extend this
as soon as possible to functions of more
than one variable. We do this in the next chapter.

%%% Line 54
\subsection{Derivatives} \label{derivatives}

%%% Line 56
The derivative of a function \(f(x)\) is another function \(f'(x)\) that
measures how fast \(f(x)\) changes as \(x\) changes.   The derivative
has a simple geometric definition:

%%% Line 60
\begin{equation}
f'(x) = \text{slope of the tangent line  at $(x,f(x))$}
\end{equation}

%%% Line 63
To understand this, look at the graph of
the function \(f(x)\) in Figure 1 below. There you see
the tangent line \(AB\) to the graph drawn through a point \(P = (x,f(x))\). The slope of the tangent line is its \index{rise}\textit{rise} divided
by its \index{run}\textit{run}.   You can read these numbers off from the triangle \(PQR\):

%%% Line 68
\begin{equation}
f'(x) = \frac{\text{rise}}{\text{run}} = \frac{3.0}{4.5} = 0.67
\end{equation}

%%% Line 71
The derivative = slope = rise/run is a measure of how rapidly \(f(x)\)
changes as \(x\) increases. If the derivative is positive, as in
the figure below, \(f(x)\) increases as \(x\) increases.   If the derivative
is negative, \(f(x)\) decreases as \(x\) increases.

%%% Line 77
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5833333333333334\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/fa46a52c-79cc-4679-d90f-8006c8eb2b00/public}
  \caption{Figure 1. Derivative as slope}
  \label{fig:figure1}
\end{figure}

%%% Line 81
If we have a formula for a function, it is natural
to ask for a formula for its derivative. We will see how
to get such formulas in the next chapter.   There we will learn, for example, that the derivative of the quadratic function \(f(x) = ax^2 + bx + x\) is the function \(f'(x) = 2ax + b\).

%%% Line 85
\begin{problem}
Consider the graph of a function \(f(x)\) below.   Calculate
the derivative of \(f\) at   \(x = 4, 6, 8\).   Do this by drawing little triangles and measuring the rise and run, as in the example above. (If you want, click on the image to open it in a new tab, then print it).
\end{problem}

%%% Line 89
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5833333333333334\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/183e307c-10c7-4333-37d2-d0c1e283cd00/public}
  \caption{Figure 1. Derivative as slope}
  \label{fig:figure1}
\end{figure}

%%% Line 93
\subsection{Differentiable functions} \label{differentiable-functions}

%%% Line 95
A function is \index{differentiable}\textit{differentiable} if its derivative exists for all
\(x\) for which \(f\) is defined.   The function pictured in Figure 1 above is differentiable, as is any polynomial function, e.g.,   \(f(x) = x^2 + 2x +5\). On the other hand, the function whoe graph you see in the figure below
is not differentable at \(x = 0\), the "point of the vee." The function
is the absolute value function \(f(x) = |x|\). We will come back to it
in the next chapter, when we give a rigorous definition of the derivative.

%%% Line 103
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/a2167390-f701-4fae-e3cc-62abe1036200/public}
  \caption{Figure 2. Non-differentiable function}
  \label{fig:figure2}
\end{figure}

%%% Line 108
The slope of the graph above is \(-1\) for \(x < 0\) and is \(+1\) for \(x > 0\).
The abrupt change in the slope at \(x = 0\) what makes the function non-differentiable there.

%%% Line 114
A twice differentiable function has the nice property that
as we move along its graph, its tangent line turns with no sudden jumps.

%%% Line 118
\begin{problem}
For what values of \(x\) is the function pictured below not differentiable?
On which intervals is it convex?
\end{problem}

%%% Line 123
\begin{center}
\includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/24ff5ec6-e668-418b-2138-c9231bb7a400/public}
\end{center}

%%% Line 127
Unless otherwise specified, we will assume that all the functions we
use in these notes are \index{twice differentiable}\textit{twice differentiable}.   That means that the derivative of \(f\) exists
at every point where \(f\) is defined, and so does the derivative of the derivative, the so-called \index{second derivative}\textit{second derivative} \(f''(x)\).   All polynomials, exponentials, and products of exponentials   are twice differentiable.   In fact, they are \index{infinitely differentiable}\textit{infinitely differentiable}: you can calculate the derivative, the
derivative of the derivative, the derivative of the derivative of the derivative, and so on.

%%% Line 133
\begin{problem}
Consider the quadratic function \(f(x) = ax^2 + bx + c\).   Its
derivative is \(f'(x) = 2ax + b\).   What is its second derivative \(f''(x)\)?
\end{problem}

%%% Line 138
\subsection{Increasing functions} \label{increasing-functions}

%%% Line 140
We all have an intuitive
idea of what it means for a function to be increasing. For example, in Figure 1, the function \(f\) is increasing on the interval \(x > 3\) and is decreasing
on the interval \(x < 3\).
An intuitive understanding is good, indeed valuable.   But we also need a formal definition:

%%% Line 145
\begin{definition}
A function \(f\) is increasing on an interval \(a < x < b\)
if for all \(x\) and \(x'\) in the interval
 such that \(x < x'\), we have \(f(x) < f(x')\).
\end{definition}

%%% Line 151
Loosely speaking, we say that \(f\) is increasing on the interval when

%%% Line 153
\begin{indent}
\textit{\(x'\) is bigger than \(x\) implies that \(f(x')\) is bigger than
\(f(x)\).}
\end{indent}

%%% Line 160
One of the most useful properties of the derivative is that
its \index{sign}\textit{sign} tells us whether the function is increasing or decreasing:

%%% Line 163
\begin{theorem}
If \(f\) is a   twice differentiable function and if \(f'(a) > 0\),
then \(f(x)\) is increasing for all \(x\)   suffciently near to \(x = a\).
\end{theorem}

%%% Line 168
Mathematicians' favorite name for a "small quantity" is the
Greek letter \(\epsilon\), pronounced \textit{epsilon}.
"Sufficiently near" means that there is some number \(\epsilon > 0\) which
 such that \(f(x)\) is increasing on the
interval \(a - \epsilon < x < a + \epsilon\). The whole point of this
epsilon business is that without more information, we can't say
more about how big epsilon is.   In the case of the function of
Figure 1, we do have more inforamation, so we can take \(\epsilon = 2.5\).   The function
is increasing on the interval \(3 < x < 8\).

%%% Line 180
\subsection{Minima and Maxima} \label{minima-and-maxima}

%%% Line 182
In the last section we saw that if the derivative
of a twice differentiable function is positive at \(x = a\)
then it is increasing near \(a\) and that if the derivative
is negative, then the function is decreasing near \(x = a\).
What happens if the derivative is zero?   There are
three possibilities, all illustrated in the figure below:
 minimum,   maximum, or inflection point. In (a), the minimum
 value is an \index{absolute minimum}\textit{absolute minimum}: it is the smallest
 of all values of the function.   In (b), there are two minima.
 the ifrst one is an absolute minimum, the second is a \index{local minimum}\textit{local minimum}: it is the smallest of all nearby values.   In (c)
 there is an \index{inflection point}\textit{inflection point}.   At an infection point a function turns from increasing to decreasing (or \textit{vice versa}.)
 At all these so-called \index{critical points}\textit{critical points}, the derivative vanishes.   \textit{Geometrically,
 thai means that the tangent line is horizontal.}

%%% Line 197
\begin{center}
\includegraphics[width=0.51\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/b1ce5ac8-7ce6-4db1-80b8-6a38af9f2200/public}
\end{center}

%%% Line 200
There is something else that we can read from these graphs.
In (a), the case of an absolute minimum, the graph is convex
and the second derivative is positive.   In (b) the fuction is
convex near the two local minima.   In (c) the second derivative
is zero at the inflection point.   What can you say about the
graph of (b) at the maxiumum?

%%% Line 207
\textbf{Example Problem}

%%% Line 209
Let's do an example.
Suppose that you have a piece of wire \(L\)
units long.   You want to bend it into a rectangle that
encloses the largest possible area.   A rectangle has
a width \(w\) and and a height \(h\).   We know that
\(L = 2w + 2h\) and that the area is \(A = wh\).   It looks
like we have a minimization problem for a function \(A(w,h)\)
of two variables.   But we can solve for one of the
variables in terms of the other using the first equation:
\(h = L/2 - w\). Substituting into the formula for the
area, we obtain \(A = wL/2 - w^2\).   To find the maximum
area, we compute the derivative and set it to zero.   Using the
formula for the derivative of a quadratic function (section \ref{derivatives}), we have \(A'(w) = L/2 - 2w\). Solving the equation
\(A'(w) = 0\), we obtain \(w = L/4\).   Therefore \(h = L/4\), as well, and the shape in question is a square.

%%% Line 225
\subsection{Second derivative} \label{second-derivative}

%%% Line 227
Remember that we are working in the context of functions that are twice differentiable:
wherever the function \(f(x)\) is defined, so is its derivative, and also the derivative
of the derivative, or \index{second derivative}\textit{second derivative}, the second derivative
as \(f''(x)\).

%%% Line 232
What does the second derivative tell us? Consider a function such that \(f''(x)\) is positive for all \(x\) in
an interval \(a \le x \le b\). According to the discussion of section \ref{increasing-functions}, this means the first derivative is increasing.   To say that the
first derivative is increasing is to say that the slope of the tangent line
is increasing as we travel from left to right.   That is, it is turning up.   Therefore graph is curved upwards, as in the figure below. (Notice how the slope of tangent line 2 is bigger than the slope of tangent line 1, and
he slope of tangent line 3 is bigger than the slope of tangent line 2.)

%%% Line 238
\begin{center}
\includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/35feb608-562e-492b-f51c-be8533740000/public}
\end{center}

%%% Line 243
Function with positive second derivative are \index{convex}\textit{convex} or (very roughly) "bowl-shaped."   The technical defintion is that the line drawn between
points \(A\) and \(B\) of the graph lies above the graph between \(A\) and \(B\),
as in the figure below.   The line joining \(A\) to \(B\) is the \index{secant line}\textit{secant line}.   The term comes from the Latin \textit{secare}, "to cut."

%%% Line 247
\begin{center}
\includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/212555fa-12d0-4616-3613-6aef577f8900/public}
%%% Line 250
 Our example suggests (but does not prove) the following result:
\end{center}

%%% Line 252
\begin{theorem}
A twice-differentiable function for which the second derivative is positive on an interval \(a < x < b\) is convex on that interval.
\end{theorem}

%%% Line 256
Consider once again a quadratic function \(f(x) = ax^2 + b + c\).   Its derivative
is \(f'(x) = 2ax + b\) and its second derivative is   \(f''(x) = 2a\).   Thererfore
\(f(x)\) is a convex function if and only if \(a > 0\).

%%% Line 261
\subsection{Gradient descent} \label{gradient-descent}

%%% Line 263
One of the main uses of the derivative in data science
is \index{optimization}\textit{optimization}: finding the minimum (or maximum) values of functions.   According
to our discussion of minima and maxima in section \ref{minima-and-maxima},
one way to do this is to solve the equation \(f'(x) = 0\).   Unfortunately,
being able to compute the derivative doesn't mean it is easy to solve
the equation \(f'(x) = 0\).   It gets worse for functions of more than one variable where we need to solve systems of equations to find
the critical points.   What is needed instead is a way to
find good \index{approximations}\textit{approximations} of the minima without having to solve equations for them.
We will describe such a method here for functions of one variable. As we shall see in the next chapter, the
method works perfectly well for functions
of many variables.

%%% Line 275
The method is called \index{gradient descent}\textit{gradient descent}.   The idea is as follows.   Start
at some point \(x_0\) where the function has value \(y_0 = f(x_0)\).   Think of
\(x_0\) as giving the location of an approximate minimum value \(y_0\).   Our immediate
goal is to find a somewhat better approximation by moving a certain distance
to the left or right.   But which direction to choose?   The derivative \(f'(x_0)\)
will tell us which way.   We move to the
left if \(f'(x_0)\) is positive and move to the right if \(f'(x_0)\) is negative. If we do that, the value of \(f\) decreases, decreasing
the distance to the minimum.
We also need to decide how far to move.   For this we choose a number \(\eta > 0\),
the \index{learning rate}\textit{learning rate}, and set

%%% Line 286
\begin{equation}
x_1 = x_0 - \eta f'(x_0)
\end{equation}

%%% Line 289
Read this equation as: \textit{compute\(x_1\) by moving an amount \(dx = \eta|f'(x_0)|\) from
\(x_0\) in the direction in which \(f\) is decreasing.} Given an approximation to the location of the minimum \(x_n\), we can always improve it by computing

%%% Line 292
\begin{equation}
x_{n+1} = x_n - \eta f'(x_n)
\end{equation}

%%% Line 295
Very good!   We know how to improve the approximation, but
how do we know when to stop? One answer is to stop when successive approximations
differ by less than some prescribed tolerance \(\epsilon > 0\):

%%% Line 299
\begin{indent}
Stop if \(|x_{n+1} - x_n| < \epsilon\)
\end{indent}

%%% Line 302
But it may happen that the quantity \(|x_{n+1} - x_n|\) never gets small enough, or it may happen that we simply run out patience.
Therefore we amend the rule:

%%% Line 305
\begin{indent}
Stop if \(|x_{n+1} - x_n| < \epsilon\) or if \(n > \text{max\_iterations}\)
\end{indent}

%%% Line 308
To see how this works, let's compute the sequence \(x_0, x_1, x_2, \ldots\) either by hand, or (better) using
a computer program. We will do this for the function \(f(x) = x^2\).
Of course, we already know the answer: the minimum value is \(y = 0\),
which occurs at \(x = 0\).   By running a known case, we can tell
whether or not the algorithm works. Let's take the initial value to be \(x_0 = 2\), the maximumn number
of iterations \(n\) to be 1000, with a tolerance of \(0.003\).
Below is   the result of running   a small Python program (see below) to do the computation.      The last column
displays the change in \(x\), \(\Delta x = x_{n+1} - x_n\):

%%% Line 317
%%% export of this block is unimplemented

%%% Line 329
Below is a visual representation of what is going on.   In
the left panel the red dots sliding down from \((2.0, 4.0)\)
give the values \((x_n, f(x_n))\).   In the right panel you see \(x_n\)
plotted against \(n\).

%%% Line 334
\begin{figure}[h]
  \centering
  \includegraphics[width=0.51\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/0e486899-d6c7-40a6-25df-46d48af11800/public}
  \caption{Gradient Descent}
  \label{fig:gradientdescent}
\end{figure}

%%% Line 337
Here is the Python program that produced the table   Run it from the command line with \lstinline|python gradient-descent-simplest.py|.

%%% Line 339
\par\vspace{3.75pt}

%%% Line 341
\begin{verbatim}
  # file: gradient-descent-simplest.py
%%% Line 344
   import numpy as np
%%% Line 346
   def f(x):
          """The function we want to minimize: f(x) = x^2"""
          return x**2
%%% Line 350
   def df(x):
          """The derivative of f(x): f'(x) = 2x"""
          return 2*x
%%% Line 354
\section{Initial conditions
x = 2.0                                                                                                         # Starting point
learning\\_rate = 0.1   # Step size
n\\_iterations = 20          # Number of iterations} \label{initial-conditions-x--20--starting-point-learningrate--01--step-size-niterations--20--number-of-iterations}
%%% Line 359
\section{Print header
print("n                     x\\_n          f(x\\_n)      Δx\\_n")
print("-" * 25)} \label{print-header-printn-xn-fxn-xn-print---25}
%%% Line 363
   x\_prev = x
print(f"0      {x:.3f}                                    ---")
%%% Line 366
\section{Gradient descent
for n in range(1, n\\_iterations + 1):
          x\\_new = x - learning\\_rate * df(x)
          delta = x\\_new - x
          print(f"{n:2d}      {x\\_new:.3f}      {f(x\\_new):.3f}      {delta:.3f}")
          x = x\\_new} \label{gradient-descent-for-n-in-range1-niterations--1-xnew--x---learningrate--dfx-delta--xnew---x-printfn2d-xnew3f-fxnew3f-delta3f-x--xnew}
\end{verbatim}

%%% Line 373
\par\vspace{5pt}

%%% Line 375
\begin{problem}
Try a few steps of the gradient descent method with
\(f(x) = x^2 + x\).   Is it producing a sequence \(\{ x_n \}\)
that converges rapidly to the minimum?
\end{problem}

%%% Line 380
The gradient descent method does not always work.   There are, however, theorems
that give conditions under which it does. Here is one:

%%% Line 383
\begin{theorem}
Suppose that the function   \(f\)   is convex and twice continuously differentiable.   Suppose also that \(m < f''(x) < L\), where \(m > 0\).
Then gradient descent with step size
satisfying \(0 < \alpha < 2/L\) converges to the unique global minimum of \(f\).
\end{theorem}

%%% Line 388
\begin{remark}
Often times we work with functions for which the derivative
is difficult to or impossible to compute.   In those cases
we can use an approximate derivative.   Consider the secant line
\(AB\) in the figure below.   Its rise is
\end{remark}

%%% Line 394
\begin{equation}
\Delta f(x) = f(x + h/2) - f(x -h/2)
\end{equation}

%%% Line 397
and its run in \(h\).   Therefore its slope is

%%% Line 399
\begin{equation}
\text{slope of secant $AB$} = \frac{\Delta f(x)}{h} = \frac{f(x + h/2) - f(x -h/2)}{h}
\end{equation}

%%% Line 402
Consider also the tangent line to the graph a \(C\).   Then

%%% Line 404
\begin{equation}
f'(x) = \text{slope of the tangent line at $C$.}
\end{equation}

%%% Line 407
From the graph, it appears that the slope of the secant line \(AB\)
differs only a little from the slope of the tangent line at \(C\)

%%% Line 410
The idea   is this.   If we can compute values of \(f\), we
can also compute \(\Delta f(x)\).   Now look at the figure below.
\(\Delta f(x)\) is the slope of the secant line \(AB\).   When \(h\)
is small, the slope of the secant line is a good approximation to the   slope of that tangent line at \(C\).   The smaller \(h\) is,
the better the approximation.   This is also the idea behind
the formal definition of the derivative.

%%% Line 418
\begin{center}
\includegraphics[width=1\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/34f1934a-ab11-484e-edd0-36215c010f00/public}
\end{center}

%%% Line 421
\begin{remark}
We are at the very beginning of our mathematiics + data science
journey.   But here is something to note: even though you
may not yet be able to compute derivatives, you know
\textbf{what} they are, you know something about \textbf{how} they are
used, and you can \textbf{understand} theorems that speak to when
common algorithms can be expected to provide good answers.
Lastly, you have seen how one such algorithm can be \textbf{implemented}
in code.
Onward!
\end{remark}

%%% Line 432
\subsection{Integrals} \label{integrals}

%%% Line 434
In a word, \textit{integral is area}.   To be precise, consider a
function \(f(x)\) that takes positive values,
as in the Figure below.   Consider the region \(R\) which you see
there.   It is defined by two inequalities.   A point \((x,y)\)
is in \(R\) if

%%% Line 440
\begin{align}
0 \le\ & x \le b\\
0 \le\ & y \le f(x)
\end{align}

%%% Line 444
For the integral, look at Figure 2.   The \index{definite integral}\textit{definite integral} of \(f\) from \(a\) to \(b\) is the area of the region \(R\). We can decode
what we see in the figure by a pair of inequalities:

%%% Line 447
\begin{equation}
R  = \sett{(x,y)}{a \le x \le b,\quad 0 \le y \le f(x)}
\end{equation}

%%% Line 450
Like the derivative, the integral has a special notation:

%%% Line 452
\begin{equation}
\text{area}(R) = \int_a^b f(x) dx
\end{equation}

%%% Line 456
We don't yet have the tools for computing integrals of functions
given to us by a formula, but we can make do with a graphical
method for the moment, just as we did for the derivative.   Assuming that we have arranged our graph so that each little square is one unit by one unit
of measure, we can count squares and estimate partial squares in order to estimate the area.

%%% Line 462
For example, just to the right of the line \(x = a\), we
see that there are three squares which lie entirely in \(R\) and just a small
part --- maybe one tenth --- of a fourth square.   Our estimate for the area of the part of
\(R\) satisfying \(2 \le x \le 3\) is therefore \(3.1\) We do this for the rest of \(R\) and add up the reults to get

%%% Line 467
\begin{equation}
\int_a^b f(x)dx \approx 20.7.
\end{equation}

%%% Line 470
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5833333333333334\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/eea257c6-10f9-4db9-20cb-f93a7d727a00/public}
  \caption{Figure 2. Integral as area}
  \label{fig:figure2}
\end{figure}

%%% Line 473
\begin{remark}
You might ask: can we get more accurate answers? One way to do
counter squares and partial squares as beforem but to use
a smaller grid of squares.   As the squares get smaller
and smaller, the approximation gets better and better.   We
say that the true integral is the \textbf{\index{limit}\textit{limit}} of the approximate
integrals as the grid size gets smaller and smaller.   Let
\(R_n\) be region made of grid squares and partial squares.
Suppose the maximum size of a square is \(1/n\). Then
\end{remark}

%%% Line 483
\begin{equation}
\int_a^b f(x) dx = \lim_{n \to 0} \text{area}(R_n)
\end{equation}

%%% Line 486
This idea of appromation and limit occurs throughout calculus.
We will use it in the next chapter both to define the
derivative and to define definite integrals.

%%% Line 490
\subsection{Probability distributions} \label{probability-distributions}

%%% Line 492
One of the main uses of the integral in data science is in probability
theory. We will talk abou this much later, but for now consider the following problem.   An astronomer observes a star which for which the expected
position at a given time, date, and place can be calculated. But the star
always appears displaced from the expected position by an angle \(\Delta \theta_x\)
along the horizontal axis and \(\Delta \theta_y\) along the vertical axis.   Lerge
deviations from what is expected are less common than small ones.   In fact,
these deviations follow a so-called \index{normal}\textit{normal} or \index{Gaussian}\textit{Gaussian}
distribution.   Here is what this means.   There is a function \(p(\theta)\)
called a \index{probability distribution}\textit{probability distribution}.   It is a positive (non-negative) function satisfying

%%% Line 502
\begin{equation}
\int_{-\infty}^\infty p(\theta) d\theta = 1
\end{equation}

%%% Line 505
The probability that the error \(\Delta \theta\) is less than \(b\) but greater than \(a\) is the integral of the probability distribution on this interval:

%%% Line 507
\begin{equation}
P(a \le \Delta\theta_x \le b) = \int_a^b p(\theta) d\theta
\end{equation}

%%% Line 511
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5833333333333334\textwidth]{https://media.geeksforgeeks.org/wp-content/uploads/20230828135632/Probability-Density-Function.png}
  \caption{Figure 1. Probability distribution}
  \label{fig:figure1}
\end{figure}

%%% Line 514
The German mathematician Carl Friederich Gauss discovered the
probability distribution which governs these angular errors.   This distribtuion has the general form

%%% Line 517
\begin{equation}
p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
\end{equation}

%%% Line 521
Here \(\exp(x)\) is \(e^x\), \(\mu\) is the \index{mean}\textit{mean} of the distribution and \(\sigma\) is
the \index{standard deviation}\textit{standard deviation}.   The mean tells us where the distribution
is centered.   The standard deviation tells us to what extent the distribution is spread out. This formula just gven defines the \index{standard}\textit{standard} or \index{normal}\textit{normal} distribution. It is one the most commonly
used distribitions, but beware: it does not apply to everything.

%%% Line 527
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6666666666666666\textwidth]{https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/640px-Standard_deviation_diagram.svg.png}
  \caption{Standard deviation diagram}
  \label{fig:standarddeviation}
\end{figure}

%%% Line 531
\subsection{Summary} \label{summary}

%%% Line 533
Derivative is slope.

%%% Line 535
Integral is area.

%%% Line 538
\section{Linear Algebra} \label{linear-algebra}

%%% Line 541
\subsection{Vectors} \label{vectors}

%%% Line 545
The fundamental notions of linear algebra are \textit{vector} and
\textit{matrix}.   An \(n\)-dimensional vector is a   list of real numbers:

%%% Line 548
\begin{equation}
\bv = (v_1, v_2, \ldots ,v_n)
\end{equation}

%%% Line 551
The set of all such vectors, where the \(v_i\) range
over the real numbers \(\reals\) is written

%%% Line 554
\begin{equation}
\reals^n = \sett{ (v_1, v_2, \ldots ,v_n) }{ v_i \in \reals }
\end{equation}

%%% Line 557
When \(n = 2\), \(n\)-space is the Cartesian plane, as
in the figure below.   The vector \((x_1, x_2) = (4,2)\)
corresponds to the point \(P\): it is four units to the
right of the origin \(O\) and is two units up.   Here
we think of \(\bfx = (x_1, x_2)\) as a \index{position vector}\textit{position vector}.

%%% Line 564
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6333333333333333\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/ccdf9c04-5e8f-4a57-9291-d3fdb0133b00/public}
  \caption{Position vector in 2-space}
  \label{fig:positionvector}
\end{figure}

%%% Line 571
We can visualize position vectors in 3-space in the same way:
set up three perpendicular axes, and use them to locate
a point by giving three numbers, one for each axis.   See the figure below.

%%% Line 576
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/386570a0-e841-4dc9-e74d-2d4eaaa0dc00/public}
  \caption{Position vector in 3-space}
  \label{fig:positionvector}
\end{figure}

%%% Line 580
The typical problem in data science involves
vectors in \(n\)-space where the number of dimensions
(things \textit{measured}) is much larger --- ten, a thousand, maybe
more. In dimension greater than three, our vision fails us,
so we proceed by analogy. There are vector formulas for notions
like distance and angle in 2-space and 3-space, for object,
like lines and planes, circles and spheres. Because these formulas
work in \(n\)-space, so do the notions just described.

%%% Line 589
Let's begin with length. The length of a vector \(\bfx = (x_1, x_2)\)
in 2-space is given by

%%% Line 592
\begin{equation}
|| x || = \sqrt{ x_1^2 + x_2^2 }
\end{equation}

%%% Line 595
This is the Pythagorean theorem.   In the figure, we see that
the length or \index{norm}\textit{norm} if the vector \(\bfx = (3,4)\) is
\(|| \bfx || = 5\).

%%% Line 599
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6333333333333333\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/ccdf9c04-5e8f-4a57-9291-d3fdb0133b00/public}
  \caption{Position vector in 2-space}
  \label{fig:positionvector}
\end{figure}

%%% Line 602
The length of a vector in \(n\)-space is
defined in the same way:

%%% Line 605
\begin{equation}
|| x || = \sqrt{ x_1^2 + x_2^2  + \cdots + x_n^2}
\end{equation}

%%% Line 609
The numbers \(x_i\) are called the \index{components}\textit{components} or
\index{coordinates}\textit{coordinates} of the vector.

%%% Line 613
\subsection{Distance, sum and difference} \label{distance-sum-and-difference}

%%% Line 616
Once we have a notion of length, we have a notion
of \index{distance}\textit{distance}.   Suppose \(P\) and \(Q\), as in the figure
below, are points in 2-space with position vectors \(\bfx = (x_1, x_2)\) and \(\by = (y_1, y_2)\).   The distance from \(P\) to \(Q\) is

%%% Line 620
\begin{equation}
d(P,Q) = \sqrt{(y_1 - x_1)^2 + (y_2 - x_2)^2}
\end{equation}

%%% Line 623
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6333333333333333\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/232f19d6-44fc-4ed4-edc5-a579b0d1cb00/public}
  \caption{Distance between two points in 2-space}
  \label{fig:distancebetween}
\end{figure}

%%% Line 626
Once again, the formula makes sense for vectors in \(n\)-space:

%%% Line 628
\begin{equation}
d(P,Q) = \sqrt{(y_1 - x_1)^2 + (y_2 - x_2)^2 + \cdots + (y_n - x_n)^2}
\end{equation}

%%% Line 631
Notice that the terms of appearing in this formula look like
the components of a vector.   Indeed, we can define the
\index{difference}\textit{difference} of vectors like this:

%%% Line 635
\begin{equation}
\by - \bfx = (y_1 - x_1, y_2 - x_2, \ldots , y_n - x_n)
\end{equation}

%%% Line 638
Then

%%% Line 640
\begin{equation}
d(P,Q) = || \bfx - \by ||
\end{equation}

%%% Line 643
The difference of two vectors in \(n\)-space is the vector whose
components are differences of corresponding components:
\((\by - \bfx)_i = y_i - x_i\). In the same way we define the
sum of vectors by adding their components:

%%% Line 648
\begin{equation}
\by + \bfx = (y_1 + x_1, y_2 + x_2, \ldots , y_n + x_n)
\end{equation}

%%% Line 651
\subsection{Dot Product and Angle} \label{dot-product-and-angle}

%%% Line 653
So far we have defined length and distance in \(n\)-space.   Next,
let's do angle.   For this we need   the \index{dot product}\textit{dot product} of
vectors,

%%% Line 657
\begin{equation}
\bfx \cdot \by = x_1y_1 + x_2y_2 + \cdots + x_ny_n
\end{equation}

%%% Line 660
The dot product of vectors is not a vector, but
rather a \index{scalar}\textit{scalar}, that is, a number.

%%% Line 664
Consider now two vectors \(\bfx\) and \(\by\), as in the figure
below.   They enclose an angle \(\theta\).   The two vectors
are sides a triangle, where the third side runs from the
head of one of the arrows to the head of the other.   The
third side is the vector \(\by - \bfx\)   (or rather that vector
moved parallel to itself so that its tail rests on the head of \(\bfx\)).   Let's compute the length of the "opposite side:"

%%% Line 672
\begin{align}
||\by - \bfx ||^2 &= (\by - \bfx) \cdot (\by -\bfx )\\
&= ||\by||^2 + ||\bfx||^2 - 2(\by\cdot \bfx)
\end{align}

%%% Line 677
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6333333333333333\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/232f19d6-44fc-4ed4-edc5-a579b0d1cb00/public}
  \caption{Distance between two points in 2-space}
  \label{fig:distancebetween}
\end{figure}

%%% Line 681
In the case \(n = 2\) we can compute the length of the opposite
side in another way, using the \index{Law of Cosines}\textit{Law of Cosines}:

%%% Line 684
\begin{equation}
|| \by - \bfx ||^2 = ||\by||^2 + ||\bfx||^2- 2||\by||\cdot ||\bfx|| \cos\theta
\end{equation}

%%% Line 688
Comparing the last two equations, we find that

%%% Line 690
\begin{equation}
\cos\theta = \frac{\bfx \cdot \by}{||\bfx|| \cdot ||\by||}
\end{equation}

%%% Line 695
Once again, this formula makes sense not just for \(2\)-space,
But for \(n\)-space.   We take it as the deinition of angle
in higher-dimensional spaces.   (Remark, Appendix:   Cauchy-Schwarz inequality).

%%% Line 699
An immediate consequence of this last formula is that

%%% Line 701
\begin{proposition}
If \(\ a.b = 0\), then \(a\) and \(b\) are perpendicular.
\end{proposition}

%%% Line 704
Perpendicular vectors are also said to be \textbf{orthogonal}

%%% Line 706
\subsection{Scalar product and unit vectors} \label{scalar-product-and-unit-vectors}

%%% Line 708
Let \(c\) be a scalar and \(\bfx = (x_1, x_2, \ldots, x_n)\)
a vector.   Their \index{scalar product}\textit{scalar product}   is the vector

%%% Line 711
\begin{equation}
c\bfx = (cx_1, cx_2, \ldots, cx_n)
\end{equation}

%%% Line 714
That is, the \(i\)-th component of the scalar product
is the \(c\) times the \(i\)-th compoent of \(\bfx\).   In equations,
\((c\bfx)_i = c\bfx_i\).   The effect of multiplying a vector
by a scalar is to stretch it or shrink it, or to do one
of those things and also reverse its direction (if \(c\) is negative).

%%% Line 722
\begin{problem}
Let \(\bfx = (2,4)\). On graph paper, plot the vectors
\(\bfx\), \(2\bfx\),   \(0.5\bfx\), and \((-1)\bfx\).   In each case,
indicate whether scalar product is bigger, smaller, or reversed.
\end{problem}

%%% Line 727
\begin{problem}
Show that \(|| c\bfx ||= |c|\cdot ||\bfx||\)
\end{problem}

%%% Line 731
A \index{unit vector}\textit{unit vector} is a vector of unit length.   Given
any nonzero vector \(\bfx\), there is a unique unit vector \(\hat{\bfx}\) that points in the same direction as \(\bfx\):

%%% Line 734
\begin{equation}
\hat{\bfx} = \frac{\bfx}{|| \bfx ||}
\end{equation}

%%% Line 737
That is, \(\hat{\bfx}\) is the scalar product of \(\bfx\) and the scalar \(1/||x||\).

%%% Line 739
\begin{problem}
Find the unit vectors associated to the following vectors:
(a) \(\bfx = (3,4)\), (b) \(\bfx = (1, 1)\), (c) \(\bfx = (1,2,3,4)\).
\end{problem}

%%% Line 745
\subsection{Point Clouds and Clusters} \label{point-clouds-and-clusters}

%%% Line 747
Let us now consider an example of the kind of problem
that comes up routinely in data science. A team of
medical scientists is searching for a rapid diagnostic test
for a new disease.   To this end, they have measured the
concentrations of then chemical substances in the blood
of a group of test subjects. This data is a point   \((x_1, \ldots x_{10})\) in 10-dimensional space
\(\reals^{10}\).   Suppose that we have test results from
100 hundred subjects.   They form
a cloud of 100 points in \(\reals^{10}\) ---   a so-called \index{point cloud}\textit{point cloud}.   Suppose further that
there are two kinds of people in the test group: healthy people
chosen at random, and a random selection of people with a
certain disease.   The aim of the study is to see if the test
results can reliably determine who is healthy and who has the disease.
If it were possible to see the data with 10-dimensional eyes,
we might find that the data consisted of two clusters of points ---
the big point cloud is really two smaller clouds as in the figure below.

%%% Line 766
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/61fa5b5c-1109-4692-d353-ae559b183800/public}
  \caption{Two clusters of points in 10-space}
  \label{fig:twoclusters}
\end{figure}

%%% Line 769
How might we decribe this clustering if it exists? In the figure, we see that the points cluster inside two circles.   The first cluster is described by the inequality

%%% Line 771
\begin{equation}
d(x, A) < r_A
\end{equation}

%%% Line 774
where \(d(x,A)\) is the distance from \(x\) to center \(A\)
of the first circle, where   \(r_A\) is the
radius.   The same logic applies to 3-space, except
that we have spheres instead of circles.   Is there a way of doing something like this in \(n\)-space? While we do not yet have
an algorithm for finding clusters, we have one of the
mathematical tools needed to do just that: a notion of
distance.   We could, for example, begin like this:

\begin{enumerate}

%%% Line 782
\item Choose a point \(p\) at random. If there are at least 5 points in the cloud withn a distace 1.5 of \(p\), declare these points to be the seed of a cluster. If not, choose another point at random and begin this process again.

%%% Line 787
\item Continue adding points to the seed cluster if they are are with 1.5 units of the seed cluster.   When there are no more such points, go back to (1) and start a new cluster

\end{enumerate}

%%% Line 789
This is simplified outline of the DBSCAN algorithm. See
the Algorithms chapter for a full description.
Below is an example that uses DBSCAN.   A Python program was used to generate
300 points in 5-space with 3 clusters --- thus an artificial
point cloud
represented by a 300x5 matrix. Then the DBSCAN algorithm
from the Python scikit-learn library was run to find the clusters.

%%% Line 798
The the output of this algorithm was fed in to another algorithm which finds a good projection of the data from 5-space to 2-space
so that it can be visualized.   This phase uses PCA (principal component analysis), something we will study later. The projected data was plotted using matplotlib.

%%% Line 802
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/9b4737fc-472b-4492-71b9-461a6412cc00/public}
  \caption{Projected data from 5-space to 2-space}
  \label{fig:projecteddata}
\end{figure}

%%% Line 807
\subsection{Projections, PCA, and dimension reduction} \label{projections-pca-and-dimension-reduction}

%%% Line 809
We have already talked about projections as a way of reducing
the dimension of our data so that we can visualize and perhaps
detect patterns in it.   So far, however, our projections
were very limited in nature.   We use, for example, the
projection \(p: \reals^n \to \reals^2\) given by
the formula \(p(x_1, x_2, \ldots, x_n) = (x_1, x_2)\).   Let \(p_{ij}(x_1, x_2, \ldots, x_n) = (x_i, x_j)\) be the projection onto the
\(x_i\) and \(x_j\) axes, in that order.   This gives \(n(n-1)\) possible projections: \(n\) choices for \(i\) and \(n-1\) for \(j\). However, there are
actually infinitely many choices, some better than others.

%%% Line 818
To describe the other projections, consider the equations

%%% Line 820
\begin{align}
y_1 &= a_{11}x_1 + a_{12}x_2  + \cdots + a_{1n}x_n\\
y_2 &= a_{21}x_1 + a_{22}x_2  + \cdots + a_{2n}x_n\\
\ldots\\
y_m &= a_{m1}x_1 + a_{m2}x_2  + \cdots + a_{mn}x_n
\end{align}

%%% Line 826
They define a function \(p: \reals^n \to \reals^m\).
This is our first draft of a projection.   Note that it is
determined by the numbers \(a_{ij}\).   We can assemble them
in a rectangular table, a so-called \index{matrix}\textit{matrix}:

%%% Line 831
\begin{align}
A = \begin{pmatrix}\\
a_{11} & a_{12} & \ldots & a_{1n}\\
a_{21} & a_{22} & \ldots & a_{2n}\\
... & ... & ... & ...\\
a_{m1} & a_{m2} & \ldots & a_{mn}\\
\end{pmatrix}
\end{align}

%%% Line 839
This is an   \(m\times n\) matrix: it has \(m\) rows,
each consisting of an \(n\)-dimensional \index{row vector}\textit{row vector}

%%% Line 842
\begin{equation}
a_i = (a_{i1}, a_{i2}, \ldots a_{\in})
\end{equation}

%%% Line 846
The matrix \(A\)
contains everthing that you need to know to compute the projection
\(\by = p(\bfx)\)

%%% Line 852
There is something else to notice: The components of the output vector \(\by\) are dot products
of the row vectors \(\ba_i\) with input vector \(\bfx\):

%%% Line 855
\begin{equation}
\by_i = \ba_i \cdot \bfx
\end{equation}

%%% Line 858
To put this in context, think again about the"coordinate projections" \(p_{ij}\).
They fit into this scheme as follows.   Let \(e_i\) be the vector
with a 1 at position \(i\) and 0 in the other positions. This
is a vector of unit length pointing along the \(x_i\) axis.
The matrix \(A\) corresponding to our projection ihas two rows and \(n\) columns.   All of its entries are 0, except for a 1 in column \(i\) of the first row and a 1 in column \(j\) of the second row.   Here is the matrix corresponding to the projection \(p_{24}\):

%%% Line 865
\begin{align}
A_{24} = \begin{pmatrix}\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
\end{pmatrix}
\end{align}

%%% Line 871
What is important about this matrix is not the 1's and 0's
but rather that its two rows are
vectors of   unit length that meet at right angles. How do
we know this? Let \(\bu\) and \(\bv\) be the first and
second rows of the matrix \(A\).   Then \(\bu\cdot\bu = 1\),
so \(||\bu||   = 1\).   A vector of unit length is called a
\index{unit vector}\textit{unit vector}.   Note also that \(\bu \cdot \bv = 0\).
Therefore the cosine af the angle is zero.   This means
that the angle between \(\bu\) and \(\bv\) is either \(\pi/2\) or \(3\pi/2\).   In either case, they meet at a right angle.

%%% Line 881
Let's give very simple example to show how this freedom
to choose projections can help us.   Below is a point cloud
consisting of 2D data plotted in the usual \(x_1, x_2\) coordinates.
Each axis corresponds to a measurement of some definite kind.

%%% Line 886
We see a small point cloud with two kinds of points --- red and
green.   If we project onto the \(OA\) axis, the two point clouds
are not resolved: one is inside the other. However, let's rotate
to the OC, OD axes, then project onto the OC axis. This time
the two point clouds are cleanly to the left, the greens to the right.   An exercise in trigonometry tells us that this projection is defined by the function

%%% Line 892
\begin{equation}
q(x_1, x_2) = x_1\cos\theta  + x_2 \sin\theta
\end{equation}

%%% Line 895
where \(\theta\) is the angle from \(OA\) to \(OC\).   Note that
\(\bu = (\cos\theta, \sin\theta)\) is a unit vector, so

%%% Line 899
\begin{equation}
q(\bfx) = \bfx\cdot \bu
\end{equation}

%%% Line 902
fits the general pattern.

%%% Line 905
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/2c91b93a-ab13-43a9-34c2-97eca7d41600/public}
  \caption{Projection of points in 2-space}
  \label{fig:projectionof}
\end{figure}

%%% Line 908
The big question, of course, is how do we find
these good projections.   That is what the Principal
Component Algorithm (PCA) is for. The theoretical
requirements for it are the notions of eigenvectors
and eigenvalues of a symmetric matrix, something we will study in XX. The projection matrix given to us by PCA will
always be a matrix whose rows are orthogonal unit vectors.

%%% Line 915
\subsection{Matrices} \label{matrices}

%%% Line 917
The notion of matrix arose naturally in the previous section
as a way to summarize the data needed to define a projection.
Let's talk a more about these objects.   Consider by way of
example the matrix

%%% Line 922
\begin{align}
A = \begin{pmatrix}\\
1 &-2 &\phantom{-}0 &3\\
0 &\phantom{-}1 &-1 &1\\
\end{pmatrix}
\end{align}

%%% Line 928
Consider also the \index{column vector}\textit{column vector}

%%% Line 930
\begin{equation}
\bfx = \begin{pmatrix}
 1 \\
 1 \\
 1 \\
 1
 \end{pmatrix}
\end{equation}

%%% Line 938
Let \(\by\) be the column vector whose components are the
dot products of the rows of \(A\) with \(\bfx\):

%%% Line 941
\begin{equation}
\by = \begin{pmatrix}
  {\ba}_1\cdot \bfx \\
  {\ba}_2\cdot \bfx
\end{pmatrix}
= \begin{pmatrix}
  2 \ \\
  1
\end{pmatrix}
\end{equation}

%%% Line 951
We can write this more compactly as the \index{matrix product}\textit{matrix product}

%%% Line 953
\begin{equation}
\by = A\bfx
\end{equation}

%%% Line 956
The product is defined by defining the components of \(A\bfx\):

%%% Line 958
\begin{equation}
(A\bfx)_i = \ba_i \cdot \bfx
\end{equation}

%%% Line 962
\subsection{Coordinate Changes} \label{coordinate-changes}

%%% Line 965
(( under construction ))

%%% Line 967
\begin{align}
\begin{pmatrix}\\
x_1'\\
x_2'\\
\end{pmatrix}\\
=\\
\begin{pmatrix}\\
\phantom{-}\cos\theta & \sin\theta\\
-\sin\theta & \cos\theta\\
\end{pmatrix}\\
\begin{pmatrix}\\
x_1\\
x_2\\
\end{pmatrix}
\end{align}

%%% Line 983
\subsection{Good projections: a case study} \label{good-projections-a-case-study}

%%% Line 985
We said above that problems in data science
are naturally posed in the context of
higher-dimensional spaces.   Let's look at an example
from biology.   The data consists measuremnt of
sharks teeth, both from living and fossil specimens.
In the case of fossil sharks, their teeth are often their
only remnant.
The aim of the study is categorize the teeth at the
level of species and genus and to find phylogenetic connections between those "taxa." For example, we humans are \textit{Homo sapiens} --- genus \textit{Homo} and species \textit{sapiens}, whereas Neanderthal man is \textit{Homo neanderthalenis}.   We share
a common ancestor from roughly 600,000 to 800,000 years ago.   That is our \index{phylogenetic relation}\textit{phylogenetic relation} --- our relationship in the tree of life.

%%% Line 996
The measuremnt data in the shark study are lengths and
angles of a collection of 175 fossil teeth, as illustrated by the figure below. There are fifteen of these, so each tooth
is represented by a 15-dimensional vector.   The data itself
is given by a matrix with 175 rows and 15 columns --- one row
for each tooth.

%%% Line 1002
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{https://imagedelivery.net/9U-0Y4sEzXlO6BXzTnQnYQ/7abcafac-fe48-468a-9631-ad43c58f5400/public}
  \caption{175 fossil teeth in 15-space}
  \label{fig:175fossil}
\end{figure}

%%% Line 1005
A data set such as the one at hand can be thought of as
a cloud of points in a 175-dimensional space. If it were
a cloud of points in a 2 or 3-dimensional space, we might
be able to "just look at it"   and
pick out several clusters of points, Our data is what it is,

%%% Line 1012
but recall that we can project our data from

%%% Line 1016
perhaps there is not just one cloud, but several, along
with a random sprinkling of a few points not belonging to any
of the easily distinguished clouds.   That is, perhaps the
2-D data looks like this:

%%% Line 1021
FIGURE

%%% Line 1026
Alas, it could also look like this, with no obvious clustering:

%%% Line 1028
FIGURE

%%% Line 1033
\section{Glossary} \label{glossary}

%%% Line 1035


%%% Line 1037
\section{Appendix: Mathematical Addenda} \label{appendix-mathematical-addenda}

%%% Line 1040
\subsection{Gradient Descent} \label{gradient-descent}

%%% Line 1042
(( To be read add at your own risk :-))

%%% Line 1044
Let \(\nabla f\) be the \index{gradient}\textit{gradient} of \(f\), that is, the column vector of first partial
derivatives.   Let \(\nabla^2f\) be the \index{Hessian matrix}\textit{Hessian matrix} of \(f\), that is, the matrix
of second partial derivatives.

%%% Line 1048
\begin{theorem}
If the function   \(f:\reals^n \rightarrow \reals\)   is convex and differentiable, and its gradient   \(\nabla f\)   is Lipschitz continuous with Lipschitz constant   \(L > 0\), then gradient descent with a fixed step size   \(\alpha \in (0, \frac{2}{L})\)   converges to a global minimum of   \(f\).
\end{theorem}

%%% Line 1053
\begin{theorem}
Suppose that the function   \(f:\reals^n \rightarrow \reals\)   is convex and twice continuously differentiable.   Suppose also that there are constants \(m > 0\) and
\(L > 0\) such that \(\nabla^2 f(x) - m I\) and \(\nabla^2 f(x) - L I\) are
positive-definite matrices. Then gradient descent with step size
satisfying \(0 < \alpha < 2/L\) converges to the unique global minimum of \(f\).
\end{theorem}

%%% Line 1062
\section{References} \label{references}

%%% Line 1064
\bibitem{AA} Anil Ananthasamy, \u{Why Machines Learn}

%%% Line 1066
\href{https://towardsdatascience.com/gradient-descent-algorithm-a-deep-dive-cf04e8115f21}{Gradient Descent Algorithm — a Deep Dive}

%%% Line 1068
\href{https://raphaelvallat.com/bandpower.html}{EEG Spectral Density}

%%% Line 1070
\href{https://www.osti.gov/servlets/purl/983240}{Steepest descent (Arizona)}

\clearpage

\printindex



\end{document}
